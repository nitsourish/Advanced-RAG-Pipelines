{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Graph based Hybrid RAG: Graph-Enhanced Retrieval-Augmented Generation\n",
    "\n",
    "In this notebook, I implement a hybrid/mixed RAG system, that combines the strengths of both\n",
    "\n",
    "**A) Graph RAG - Technique that enhances traditional RAG systems by organizing knowledge as a connected graph rather than a flat collection of documents.**\n",
    "\n",
    "**B) semantic vector search - Context retrival based on traditional vector search based RAG.**\n",
    "\n",
    "## Key Benefits of Graph RAG\n",
    "\n",
    "- Preserves relationships between pieces of information\n",
    "- Decomposition of structure(order of words) and signal (features) that make them so powerful\n",
    "- Enables traversal through connected concepts to find relevant context\n",
    "- Improves handling of complex, multi-part queries\n",
    "- Provides better explainability through visualized knowledge paths\n",
    "- This allows the system to navigate related concepts and retrieve more contextually relevant information than standard vector similarity approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "We begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "from openai import OpenAI\n",
    "from typing import List, Dict, Tuple, Any\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from PIL import Image\n",
    "import io\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the OpenAI API Client\n",
    "We initialize the OpenAI client to generate embeddings and responses.\n",
    "\n",
    "- An important step is to get an OPENAI_API_KEY from https://platform.openai.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client with the base URL and API key\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.openai.com/v1/\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # Retrieve the API key from environment variables\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing Functions\n",
    "\n",
    "### content extraction pipeline\n",
    "\n",
    "- Content extraction pagewise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract text content from a PDF file.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        \n",
    "    Returns:\n",
    "        str: Extracted text content\n",
    "    \"\"\"\n",
    "    print(f\"Extracting text from {pdf_path}...\")  # Print the path of the PDF being processed\n",
    "    pdf_document = fitz.open(pdf_path)  # Open the PDF file using PyMuPDF\n",
    "    text = \"\"  # Initialize an empty string to store the extracted text\n",
    "    \n",
    "    # Iterate through each page in the PDF\n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document[page_num]  # Get the page object\n",
    "        text += page.get_text()  # Extract text from the page and append to the text string\n",
    "    \n",
    "    return text  # Return the extracted text content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing Functions\n",
    "\n",
    "- Basic cleaning of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    Clean text by removing extra whitespace and special characters.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    \n",
    "    # Replace multiple whitespace characters (including newlines and tabs) with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Fix common OCR issues by replacing tab and newline characters with a space\n",
    "    text = text.replace('\\\\t', ' ')\n",
    "    text = text.replace('\\\\n', ' ')\n",
    "    \n",
    "    # Remove any leading or trailing whitespace and ensure single spaces between words\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking the Extracted Text\n",
    "Once we have the extracted text, we divide it into smaller, overlapping chunks - more manageable pieces to improve retrieval accuracy and reduce computational overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=1000, overlap=200):\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to chunk\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        overlap (int): Overlap between chunks in characters\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: List of chunks with metadata\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks\n",
    "    \n",
    "    # Iterate over the text with a step size of (chunk_size - overlap)\n",
    "    for i in range(0, len(text), chunk_size - overlap):\n",
    "        # Extract a chunk of text from the current position\n",
    "        chunk_text = text[i:i + chunk_size]\n",
    "        chunk_text = clean_text(chunk_text)\n",
    "        # Ensure we don't add empty chunks\n",
    "        if chunk_text:\n",
    "            # Append the chunk with its metadata to the list\n",
    "            chunks.append({\n",
    "                \"text\": chunk_text,  # The chunk of text\n",
    "                \"index\": len(chunks),  # The index of the chunk\n",
    "                \"start_pos\": i,  # The starting position of the chunk in the original text\n",
    "                \"end_pos\": i + len(chunk_text)  # The ending position of the chunk in the original text\n",
    "            })\n",
    "    \n",
    "    # Print the number of chunks created\n",
    "    print(f\"Created {len(chunks)} text chunks\")\n",
    "    \n",
    "    return chunks  # Return the list of chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings for Text Chunks\n",
    "Embeddings transform text into numerical vectors, which allow for efficient similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(texts, model=\"BAAI/bge-en-icl\"):\n",
    "    \"\"\"\n",
    "    Create embeddings for the given texts.\n",
    "    \n",
    "    Args:\n",
    "        texts (List[str]): Input texts\n",
    "        model (str): Embedding model name\n",
    "        \n",
    "    Returns:\n",
    "        List[List[float]]: Embedding vectors\n",
    "    \"\"\"\n",
    "    # Handle empty input\n",
    "    if not texts:\n",
    "        return []\n",
    "        \n",
    "    # Process in batches if needed (OpenAI API limits)\n",
    "    batch_size = 100\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # Iterate over the input texts in batches\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]  # Get the current batch of texts\n",
    "        \n",
    "        # Create embeddings for the current batch\n",
    "        response = client.embeddings.create(\n",
    "            model=model,\n",
    "            input=batch\n",
    "        )\n",
    "        \n",
    "        # Extract embeddings from the response\n",
    "        batch_embeddings = [item.embedding for item in response.data]\n",
    "        all_embeddings.extend(batch_embeddings)  # Add the batch embeddings to the list\n",
    "    \n",
    "    return all_embeddings  # Return all embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Vector Store\n",
    "\n",
    "- A simplified real time object based vector store implementation. An industry level application may require a permanent vector database consisting embeddings and lookup tables of entire knowledge base for faster and efficient retrieval\n",
    "\n",
    "- The VectorStore object is consist of\n",
    "   - chunk embeddings\n",
    "   - contents/texts\n",
    "   - metadata of elements\n",
    "\n",
    "\n",
    "- The VectorStore object has `similarity_search` recipe of finding the most similar items to a query embedding based on **cosine similarity** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    A simple vector store implementation using NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vectors = []  # List to store embedding vectors\n",
    "        self.texts = []  # List to store text content\n",
    "        self.metadata = []  # List to store metadata\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        Add an item to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The text content\n",
    "            embedding (List[float]): The embedding vector\n",
    "            metadata (Dict, optional): Additional metadata\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))  # Append the embedding vector\n",
    "        self.texts.append(text)  # Append the text content\n",
    "        self.metadata.append(metadata or {})  # Append the metadata (or empty dict if None)\n",
    "    \n",
    "    def add_items(self, items, embeddings):\n",
    "        \"\"\"\n",
    "        Add multiple items to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            items (List[Dict]): List of text items\n",
    "            embeddings (List[List[float]]): List of embedding vectors\n",
    "        \"\"\"\n",
    "        for i, (item, embedding) in enumerate(zip(items, embeddings)):\n",
    "            self.add_item(\n",
    "                text=item[\"text\"],  # Extract text from item\n",
    "                embedding=embedding,  # Use corresponding embedding\n",
    "                metadata={**item.get(\"metadata\", {}), \"index\": i}  # Merge item metadata with index\n",
    "            )\n",
    "    \n",
    "    def similarity_search_with_scores(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        Find the most similar items to a query embedding with similarity scores.\n",
    "        \n",
    "        Args:\n",
    "            query_embedding (List[float]): Query embedding vector\n",
    "            k (int): Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List[Tuple[Dict, float]]: Top k most similar items with scores\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # Return empty list if no vectors are stored\n",
    "        \n",
    "        # Convert query embedding to numpy array\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # Calculate similarities using cosine similarity\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = cosine_similarity(query_vector, vector.reshape(1, -1))[0][0]  # Compute cosine similarity\n",
    "            similarities.append((i, similarity))  # Append index and similarity score\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top k results with scores\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],  # Retrieve text by index\n",
    "                \"metadata\": self.metadata[idx],  # Retrieve metadata by index\n",
    "                \"similarity\": float(score)  # Add similarity score\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def get_all_documents(self):\n",
    "        \"\"\"\n",
    "        Get all documents in the store.\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict]: All documents\n",
    "        \"\"\"\n",
    "        return [{\"text\": text, \"metadata\": meta} for text, meta in zip(self.texts, self.metadata)]  # Combine texts and metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Graph Construction\n",
    "\n",
    "#### Extract important concepts in terms of key terms, entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_concepts(text):\n",
    "    \"\"\"\n",
    "    Extract key concepts from text using OpenAI's API.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to extract concepts from\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: List of concepts\n",
    "    \"\"\"\n",
    "    # System message to instruct the model on what to do\n",
    "    system_message = \"\"\"Extract key concepts and entities from the provided text.\n",
    "Return ONLY a list of 5-10 key terms, entities, or concepts that are most important in this text.\n",
    "Format your response as a JSON array of strings.\"\"\"\n",
    "\n",
    "    # Make a request to the OpenAI API\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": f\"Extract key concepts from:\\n\\n{text[:3000]}\"}  # Limit for API\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Parse concepts from the response\n",
    "        concepts_json = json.loads(response.choices[0].message.content)\n",
    "        concepts = concepts_json.get(\"concepts\", [])\n",
    "        if not concepts and \"concepts\" not in concepts_json:\n",
    "            # Try to get any array in the response\n",
    "            for key, value in concepts_json.items():\n",
    "                if isinstance(value, list):\n",
    "                    concepts = value\n",
    "                    break\n",
    "        return concepts\n",
    "    except (json.JSONDecodeError, AttributeError):\n",
    "        # Fallback if JSON parsing fails\n",
    "        content = response.choices[0].message.content\n",
    "        # Try to extract anything that looks like a list\n",
    "        matches = re.findall(r'\\[(.*?)\\]', content, re.DOTALL)\n",
    "        if matches:\n",
    "            items = re.findall(r'\"([^\"]*)\"', matches[0])\n",
    "            return items\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build knowledge graph\n",
    "\n",
    "- For each chunk from extracted concepts the pair wise similarities being calculated\n",
    "- Knowledge graph construction - Based on the concept and similarity scores graph nodes and respective edge weights being constructed  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_knowledge_graph(chunks):\n",
    "    \"\"\"\n",
    "    Build a knowledge graph from text chunks.\n",
    "    \n",
    "    Args:\n",
    "        chunks (List[Dict]): List of text chunks with metadata\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[nx.Graph, List[np.ndarray]]: The knowledge graph and chunk embeddings\n",
    "    \"\"\"\n",
    "    print(\"Building knowledge graph...\")\n",
    "    \n",
    "    # Create a graph\n",
    "    graph = nx.Graph()\n",
    "    \n",
    "    # Extract chunk texts\n",
    "    texts = [chunk[\"text\"] for chunk in chunks]\n",
    "    \n",
    "    # Create embeddings for all chunks\n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    embeddings = create_embeddings(texts)\n",
    "    \n",
    "    # Add nodes to the graph\n",
    "    print(\"Adding nodes to the graph...\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # Extract concepts from the chunk\n",
    "        print(f\"Extracting concepts for chunk {i+1}/{len(chunks)}...\")\n",
    "        concepts = extract_concepts(chunk[\"text\"])\n",
    "        print(f\"Concepts for chunk {i+1}: {concepts}\")\n",
    "        # Add node with attributes\n",
    "        graph.add_node(i, \n",
    "                      text=chunk[\"text\"], \n",
    "                      concepts=concepts,\n",
    "                      embedding=embeddings[i])\n",
    "    \n",
    "    # Connect nodes based on shared concepts\n",
    "    print(\"Creating edges between nodes...\")\n",
    "    for i in range(len(chunks)):\n",
    "        node_concepts = set(graph.nodes[i][\"concepts\"])\n",
    "        for j in range(i + 1, len(chunks)):\n",
    "            # Calculate concept overlap\n",
    "            other_concepts = set(graph.nodes[j][\"concepts\"])\n",
    "            shared_concepts = node_concepts.intersection(other_concepts)\n",
    "            \n",
    "            # If they share concepts, add an edge\n",
    "            if shared_concepts:\n",
    "                # Calculate semantic similarity using embeddings\n",
    "                similarity = np.dot(embeddings[i], embeddings[j]) / (np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[j]))\n",
    "                \n",
    "                # Calculate edge weight based on concept overlap and semantic similarity\n",
    "                concept_score = len(shared_concepts) / min(len(node_concepts), len(other_concepts))\n",
    "                edge_weight = 0.7 * similarity + 0.3 * concept_score\n",
    "                \n",
    "                # Only add edges with significant relationship\n",
    "                if edge_weight > 0.6:\n",
    "                    graph.add_edge(i, j, \n",
    "                                  weight=edge_weight,\n",
    "                                  similarity=similarity,\n",
    "                                  shared_concepts=list(shared_concepts))\n",
    "    \n",
    "    print(f\"Knowledge graph built with {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges\")\n",
    "    return graph, embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Traversal and Query Processing\n",
    "\n",
    "- Identify starting nodes to traverse with based on similarity score between query embedding and node embeddings\n",
    "\n",
    "- Traverse the constructed graph using BFS(breadth-first search) algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_graph(query, graph, embeddings, top_k=5, max_depth=3):\n",
    "    \"\"\"\n",
    "    Traverse the knowledge graph to find relevant information for the query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user's question\n",
    "        graph (nx.Graph): The knowledge graph\n",
    "        embeddings (List): List of node embeddings\n",
    "        top_k (int): Number of initial nodes to consider\n",
    "        max_depth (int): Maximum traversal depth\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Relevant information from graph traversal\n",
    "    \"\"\"\n",
    "    print(f\"Traversing graph for query: {query}\")\n",
    "    \n",
    "    # Get query embedding\n",
    "    query_embedding = create_embeddings(query)\n",
    "    \n",
    "    # Calculate similarity between query and all nodes\n",
    "    similarities = []\n",
    "    for i, node_embedding in enumerate(embeddings):\n",
    "        similarity = np.dot(query_embedding, node_embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(node_embedding))\n",
    "        similarities.append((i, similarity))\n",
    "    \n",
    "    # Sort by similarity (descending)\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get top-k most similar nodes as starting points\n",
    "    starting_nodes = [node for node, _ in similarities[:top_k]]\n",
    "    print(f\"Starting traversal from {len(starting_nodes)} nodes\")\n",
    "    \n",
    "    # Initialize traversal\n",
    "    visited = set()  # Set to keep track of visited nodes\n",
    "    traversal_path = []  # List to store the traversal path\n",
    "    results = []  # List to store the results\n",
    "    \n",
    "    # Use a priority queue for traversal\n",
    "    queue = []\n",
    "    for node in starting_nodes:\n",
    "        heapq.heappush(queue, (-similarities[node][1], node))  # Negative for max-heap\n",
    "    \n",
    "    # Traverse the graph using a modified breadth-first search with priority\n",
    "    while queue and len(results) < (top_k * 3):  # Limit results to top_k * 3\n",
    "        _, node = heapq.heappop(queue)\n",
    "        \n",
    "        if node in visited:\n",
    "            continue\n",
    "        \n",
    "        # Mark as visited\n",
    "        visited.add(node)\n",
    "        traversal_path.append(node)\n",
    "        \n",
    "        # Add current node's text to results\n",
    "        results.append({\n",
    "            \"text\": graph.nodes[node][\"text\"],\n",
    "            \"concepts\": graph.nodes[node][\"concepts\"],\n",
    "            \"node_id\": node\n",
    "        })\n",
    "        \n",
    "        # Explore neighbors if we haven't reached max depth\n",
    "        if len(traversal_path) < max_depth:\n",
    "            neighbors = [(neighbor, graph[node][neighbor][\"weight\"]) \n",
    "                        for neighbor in graph.neighbors(node)\n",
    "                        if neighbor not in visited]\n",
    "            \n",
    "            # Add neighbors to queue based on edge weight\n",
    "            for neighbor, weight in sorted(neighbors, key=lambda x: x[1], reverse=True):\n",
    "                heapq.heappush(queue, (-weight, neighbor))\n",
    "    \n",
    "    print(f\"Graph traversal found {len(results)} relevant chunks\")\n",
    "    return results, traversal_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation\n",
    "\n",
    "- Based on the query and retrieved results as context generate the final answer.\n",
    "- gpt-4o is used as LLM brain, however worth exploring _Meta’s Llama family models_\n",
    "- Also temperature plays crucial role in response generation, the degree of exploration(randomness) or exploitation(deterministic).I used a moderate temperature = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context_string(retrieved_docs):\n",
    "    \"\"\"\n",
    "\n",
    "    combine retrieved documents into a single context string for the query.\n",
    "    Args:\n",
    "        retrieved_docs (List[Dict]): List of retrieved documents with text and metadata\n",
    "    Returns:\n",
    "        str: Combined context string for the query\n",
    "    \"\"\"\n",
    "    # Create embedding for the query\n",
    "    \n",
    "    context_texts = [chunk[\"text\"] for chunk in retrieved_docs]\n",
    "    combined_context = \"\\n\\n---\\n\\n\".join(context_texts)\n",
    "\n",
    "    return combined_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, combined_context):\n",
    "    \"\"\"\n",
    "    Generate a response using the retrieved context.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user's question\n",
    "        context_chunks (List[Dict]): Relevant chunks from graph traversal\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the maximum allowed length for the context (OpenAI limit)\n",
    "    max_context = 14000\n",
    "    \n",
    "    # Truncate the combined context if it exceeds the maximum length\n",
    "    if len(combined_context) > max_context:\n",
    "        combined_context = combined_context[:max_context] + \"... [truncated]\"\n",
    "    \n",
    "    # Define the system message to guide the AI assistant\n",
    "    system_message = \"\"\"You are a helpful AI assistant. Answer the user's question based on the provided context.\n",
    "If the information is not in the context, say so. Refer to specific parts of the context in your answer when possible.\"\"\"\n",
    "\n",
    "    # Generate the response using the OpenAI API\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",  # Specify the model to use\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},  # System message to guide the assistant\n",
    "            {\"role\": \"user\", \"content\": f\"Context:\\n{combined_context}\\n\\nQuestion: {query}\"}  # User message with context and query\n",
    "        ],\n",
    "        temperature=0.2  # Set the temperature for response generation\n",
    "    )\n",
    "    \n",
    "    # Return the generated response content\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "- Visualize knowledge graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_graph_trarsveal(graph, traversal_path):\n",
    "    \"\"\"\n",
    "    Visualize the knowledge graph and the traversal path.\n",
    "    \n",
    "    Args:\n",
    "        graph (nx.Graph): The knowledge graph\n",
    "        traversal_path (List): List of nodes in traversal order\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 10))  # Set the figure size\n",
    "    \n",
    "    # Define node colors, default to light blue\n",
    "    node_color = ['lightblue'] * graph.number_of_nodes()\n",
    "    \n",
    "    # Highlight traversal path nodes in light green\n",
    "    for node in traversal_path:\n",
    "        node_color[node] = 'lightgreen'\n",
    "    \n",
    "    # Highlight start node in green and end node in red\n",
    "    if traversal_path:\n",
    "        node_color[traversal_path[0]] = 'green'\n",
    "        node_color[traversal_path[-1]] = 'red'\n",
    "    \n",
    "    # Create positions for all nodes using spring layout\n",
    "    pos = nx.spring_layout(graph, k=0.5, iterations=50, seed=42)\n",
    "    \n",
    "    # Draw the graph nodes\n",
    "    nx.draw_networkx_nodes(graph, pos, node_color=node_color, node_size=500, alpha=0.8)\n",
    "    \n",
    "    # Draw edges with width proportional to weight\n",
    "    for u, v, data in graph.edges(data=True):\n",
    "        weight = data.get('weight', 1.0)\n",
    "        nx.draw_networkx_edges(graph, pos, edgelist=[(u, v)], width=weight*2, alpha=0.6)\n",
    "    \n",
    "    # Draw traversal path with red dashed lines\n",
    "    traversal_edges = [(traversal_path[i], traversal_path[i+1]) \n",
    "                      for i in range(len(traversal_path)-1)]\n",
    "    \n",
    "    nx.draw_networkx_edges(graph, pos, edgelist=traversal_edges, \n",
    "                          width=3, alpha=0.8, edge_color='red', \n",
    "                          style='dashed', arrows=True)\n",
    "    \n",
    "    # Add labels with the first concept for each node\n",
    "    labels = {}\n",
    "    for node in graph.nodes():\n",
    "        concepts = graph.nodes[node]['concepts']\n",
    "        label = concepts[0] if concepts else f\"Node {node}\"\n",
    "        labels[node] = f\"{node}: {label}\"\n",
    "    \n",
    "    nx.draw_networkx_labels(graph, pos, labels=labels, font_size=8)\n",
    "    \n",
    "    plt.title(\"Knowledge Graph with Traversal Path\")  # Set the plot title\n",
    "    plt.axis('off')  # Turn off the axis\n",
    "    plt.tight_layout()  # Adjust layout\n",
    "    plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Hybrid RAG Pipeline\n",
    "\n",
    "- We created complete rag pipeline for following 3 types of context generation method  \n",
    "    - pure vector search(**vector_only_rag**)\n",
    "    - pure graph based(**graph_only_rag**)\n",
    "    - Combining context from both approaches(**mixed_rag**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_rag_pipeline(pdf_path, query, chunk_size=1000, chunk_overlap=200, top_k=3, method=\"vector_only_rag\"):\n",
    "    \"\"\"\n",
    "    Complete Graph RAG pipeline from document to answer.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF document\n",
    "        query (str): The user's question\n",
    "        chunk_size (int): Size of text chunks\n",
    "        chunk_overlap (int): Overlap between chunks\n",
    "        top_k (int): Number of top nodes to consider for traversal\n",
    "        method (str): Method to use for RAG (\"vector_only_rag\", \"graph_only_rag\")\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Results including answer and graph visualization data\n",
    "    \"\"\"\n",
    "    combined_context = \"\"  # Initialize combined context string\n",
    "    # Extract text from the PDF document\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Split the extracted text into overlapping chunks\n",
    "    chunks = chunk_text(text, chunk_size, chunk_overlap)\n",
    "    \n",
    "    if method == \"vector_only_rag\" or method == \"mixed_rag\":\n",
    "        \n",
    "        embeddings = create_embeddings([chunk[\"text\"] for chunk in chunks])\n",
    "        # Vector only rag context text\n",
    "        retrieved_docs_vector = vector_only_rag(query,chunks, embeddings, k=5)\n",
    "        # Extract text from the retrieved documents\n",
    "        combined_context = create_context_string(retrieved_docs_vector)\n",
    "        retrieved_docs = retrieved_docs_vector  # Use vector results as retrieved documents\n",
    "        traversal_path = []  # No traversal path for vector-only method\n",
    "        graph = None  # No graph for vector-only method\n",
    "\n",
    "    if method == \"graph_only_rag\" or method == \"mixed_rag\":\n",
    "    # Build a knowledge graph from the text chunks\n",
    "        graph, embeddings = build_knowledge_graph(chunks)\n",
    "    # Traverse the knowledge graph to find relevant information for the query\n",
    "        retrieved_docs_graph, traversal_path = traverse_graph(query, graph, embeddings, top_k)\n",
    "        combined_context = create_context_string(retrieved_docs_graph)\n",
    "        retrieved_docs = retrieved_docs_graph  # Use graph results as retrieved documents\n",
    "        # Visualize the graph traversal path\n",
    "        visualize_graph_trarsveal(graph, traversal_path)\n",
    "\n",
    "    if method == 'mixed_rag':\n",
    "        # Combine the context from both methods\n",
    "        \n",
    "        retrieved_docs_vector = [chunk[\"text\"] for chunk in retrieved_docs_vector]\n",
    "        retrieved_docs_graph = [chunk[\"text\"] for chunk in retrieved_docs_graph]\n",
    "        retrieved_docs = list(set(retrieved_docs_vector + retrieved_docs_graph))\n",
    "        combined_context = \"\\n\\n---\\n\\n\".join(retrieved_docs)\n",
    "\n",
    "    # Generate a response based on the query and the relevant chunks\n",
    "    response = generate_response(query, combined_context)\n",
    "    \n",
    "    # Return the query, response, relevant chunks, traversal path, and the graph\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"response\": response,\n",
    "        \"relevant_chunks\": retrieved_docs,\n",
    "        \"traversal_path\": traversal_path,\n",
    "        \"graph\": graph\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hybrid_rag_response(vector_result, graph_result):\n",
    "    \"\"\"\n",
    "    Run the hybrid RAG pipeline with the given PDF and query.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF document\n",
    "        query (str): The user's question\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Results including answer and graph visualization data\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Extract relevant chunks from the vector result\n",
    "\n",
    "    if 'relevant_chunks' not in vector_result:\n",
    "        vector_result['relevant_chunks'] = []\n",
    "    vector_chunks = vector_result['relevant_chunks']\n",
    "    # Extract relevant chunks from the graph result\n",
    "    if 'relevant_chunks' not in graph_result:\n",
    "        graph_result['relevant_chunks'] = []\n",
    "    graph_chunks = graph_result['relevant_chunks']\n",
    "\n",
    "    if not graph_chunks and not vector_chunks:\n",
    "        return {\n",
    "            \"query\": vector_result['query'],\n",
    "            \"response\": \"No relevant information found in the document.\",\n",
    "            \"relevant_chunks\": [],\n",
    "            \"traversal_path\": [],\n",
    "            \"graph\": None\n",
    "        }\n",
    "    elif not graph_chunks:\n",
    "        return {\n",
    "            \"query\": vector_result['query'],\n",
    "            \"response\": vector_result['response'],\n",
    "            \"relevant_chunks\": vector_chunks,\n",
    "            \"traversal_path\": [],\n",
    "            \"graph\": None\n",
    "        }\n",
    "    elif not vector_chunks:\n",
    "        return {\n",
    "            \"query\": graph_result['query'],\n",
    "            \"response\": graph_result['response'],\n",
    "            \"relevant_chunks\": graph_chunks,\n",
    "            \"traversal_path\": graph_result['traversal_path'],\n",
    "            \"graph\": graph_result['graph']\n",
    "        }    \n",
    "    \n",
    "    else:\n",
    "        retrieved_docs_vector = [chunk[\"text\"] for chunk in vector_chunks]\n",
    "        retrieved_docs_graph = [chunk[\"text\"] for chunk in graph_chunks]\n",
    "        context_text_all = list(set(retrieved_docs_vector + retrieved_docs_graph))\n",
    "        combined_context = \"\\n\\n---\\n\\n\".join(context_text_all)\n",
    "\n",
    "    response = generate_response(vector_result['query'], combined_context)\n",
    "    # Return the query, response, relevant chunks, traversal path, and the graph\n",
    "    return {\n",
    "        \"query\": vector_result['query'],\n",
    "        \"response\": response,\n",
    "        \"relevant_chunks\": vector_chunks + graph_chunks,\n",
    "        \"traversal_path\": graph_result['traversal_path'],\n",
    "        \"graph\": graph_result['graph']\n",
    "    }    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Retrieval Methods\n",
    "\n",
    "### I compared 3 retrieval methods to assess pros and cons of each\n",
    "\n",
    "- `vector_only_rag` - text-only vector store and retrieval with out graph exploration\n",
    "\n",
    "- `graph-based response` - Response from graph-only RAG\n",
    "\n",
    "- `hybrid_response` - Response using contexts from both vector_only_rag and graph-based rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_only_rag(query,chunks, embeddings, k=5):\n",
    "    \"\"\"\n",
    "    Answer a query using only vector-based RAG.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        k (int): Number of documents to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Query results\n",
    "    \"\"\"\n",
    "    # Create query embedding\n",
    "    query_embedding = create_embeddings(query)\n",
    "    \n",
    "    vector_store = SimpleVectorStore()   \n",
    "    # # Add the chunks and their embeddings to the vector store\n",
    "    vector_store.add_items(chunks, embeddings)\n",
    "    print(f\"Added {len(chunks)} items to vector store\")\n",
    "    \n",
    "    # Retrieve documents using vector-based similarity search\n",
    "    retrieved_docs = vector_store.similarity_search_with_scores(query_embedding, k=k)\n",
    "    return retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_retrieval_methods(query, pdf_path, k=5, reference_answer=None):\n",
    "    \"\"\"\n",
    "    Compare different retrieval methods for a query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        pdf_path (str): Path to the PDF document\n",
    "        k (int): Number of documents to retrieve\n",
    "        reference_answer (str, optional): Reference answer for comparison\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Comparison results\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Comparing retrieval methods for query: {query} ===\\n\")\n",
    "    \n",
    "    # Run vector-only RAG\n",
    "    print(\"\\nRunning vector-only RAG...\")\n",
    "    vector_result = complete_rag_pipeline(pdf_path, query, chunk_size=1000, chunk_overlap=200, top_k=3, method=\"vector_only_rag\")\n",
    "    \n",
    "    # Run graph RAG\n",
    "    print(\"\\nRunning graph RAG...\")\n",
    "    graph_result = complete_rag_pipeline(pdf_path, query, chunk_size=1000, chunk_overlap=200, top_k=3, method=\"graph_only_rag\")\n",
    "    \n",
    "    # Run hybrid RAG\n",
    "    print(\"\\nRunning hybrid RAG...\")\n",
    "    hybrid_result = generate_hybrid_rag_response(vector_result, graph_result)\n",
    "    \n",
    "    # Compare responses from different retrieval methods\n",
    "    print(\"\\nComparing responses...\")\n",
    "    comparison = evaluate_responses(\n",
    "        query, \n",
    "        vector_result[\"response\"],\n",
    "        graph_result[\"response\"],\n",
    "        hybrid_result[\"response\"],\n",
    "        reference_answer\n",
    "    )\n",
    "    \n",
    "    # Return the comparison results\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"vector_result\": vector_result,\n",
    "        \"graph_result\": graph_result,\n",
    "        \"hybrid_result\": hybrid_result,\n",
    "        \"comparison\": comparison\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Function\n",
    "\n",
    "### Evaluates merits of above mentioned 4 retrieval strategies.\n",
    "\n",
    "- There are different industry standard statistical metrics in NLP - BLEU, ROUGE, MRR, BERTScore etc. However in LLM world a qualitative evaluation is more relevant\n",
    "- A manual review of all the the generated answers and retrieved documents would be too time consuming and also error prone\n",
    "- I created the following frame work as qualitative evaluation\n",
    "  \n",
    "  -  __LLM as Generator__: Replace human effort by generating ___n___ nos of question-answer pairs by LLM using structured prompts from the pdf document as validation data\n",
    "  \n",
    "  (** it's possible that the system generate different sets of validation question-answer pairs each time the generator functions run.This is actually the beauty of the system and make the validation system random to navigate possible memorization)\n",
    "  \n",
    "  -  __LLM as Evaluator__: Replace human effort of qualitative evaluation by evaluating the question-answer pairs by LLM itself using well designed prompts.Further the generated evaluation clearly describe merits and drawback of the each validation query in terms of possible dimensions like __Relevance__, __Factual correctness__, __Completeness__, __Clarity and coherence__ etc. w.r.t. the respective reference answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_validation_questions_answers(pdf_path, num_question_answers=10):\n",
    "    \"\"\"\n",
    "    Generate factual and analytical question-answer pairs based on a PDF document using the gpt-4o-2-2B-IT model.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        num_question_answers (int): Number of question-answer pairs to generate.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: A list of dictionaries with question-answer pairs and their types.\n",
    "    \"\"\"\n",
    "    \n",
    "    # SYSTEM PROMPT\n",
    "    system_prompt = \"\"\"\n",
    "You are an expert in Retrieval-Augmented Generation (RAG) systems and LLM safety. Based on the given document content,\n",
    "generate question-answer pairs:\n",
    "- 5 factual questions (strictly based on text, including one on hallucination in LLMs),\n",
    "- 5 analytical questions (that require critical thinking), analytical questions should not be the direct questions from the text, \n",
    "but rather require reasoning or inference based on the content and mostly should start with 'why' and will be difficult questions.\n",
    "Atleast one analytical question should require with mathematical explanation based on the document involving attention concept\n",
    "(Attention Is All You Need).\n",
    "For each question of analytical, provide a elaborate answer based on the document content with critical thinking.\n",
    "Classify each as \"factual\" or \"analytical\".\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract text from the PDF\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "    for page in doc:\n",
    "        full_text += page.get_text()\n",
    "    doc.close()\n",
    "\n",
    "    truncated_text = full_text[:4000]  # Keep within token limit\n",
    "\n",
    "    # USER PROMPT\n",
    "    user_prompt = f\"\"\"\n",
    "Document Text:\n",
    "{truncated_text}\n",
    "\n",
    "Based on the above, generate 10 question-answer pairs.\n",
    "Each question should be clearly labeled and categorized.\n",
    "Use the following format:\n",
    "\n",
    "### Factual Questions\n",
    "1. **Question:** ...\n",
    "   - **Answer:** ...\n",
    "\n",
    "### Analytical Questions\n",
    "6. **Question:** ...\n",
    "   - **Answer:** ...\n",
    "Generate {num_question_answers} question-answer pairs based on the above content.\n",
    "   \"\"\"\n",
    "     \n",
    "    # gpt-4o model call\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.3,\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message.content\n",
    "    qa_list = []\n",
    "\n",
    "    # Parsing generated content\n",
    "    sections = re.split(r\"### (Factual|Analytical) Questions\", content)\n",
    "\n",
    "    for i in range(1, len(sections), 2):\n",
    "        question_type = sections[i].strip().lower()\n",
    "        section_text = sections[i + 1]\n",
    "\n",
    "        qa_pairs = re.findall(\n",
    "            r\"\\*\\*Question:\\*\\*\\s*(.*?)\\n\\s*-\\s*\\*\\*Answer:\\*\\*\\s*(.*?)(?=\\n\\d+\\.|\\Z)\",\n",
    "            section_text,\n",
    "            flags=re.DOTALL\n",
    "        )\n",
    "\n",
    "        for question, answer in qa_pairs:\n",
    "            qa_list.append({\n",
    "                \"question\": question.strip(),\n",
    "                \"answer\": answer.strip(),\n",
    "                \"type\": question_type\n",
    "            })\n",
    "\n",
    "    # Save results\n",
    "    with open(\"data/val.json\", \"w\") as f:\n",
    "        json.dump(qa_list, f, indent=4)\n",
    "\n",
    "    return qa_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_validation_questions_answers(pdf_path, num_question_answers=10):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate question-answer pairs by analyzing the content of a PDF document.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): path to the PDF file\n",
    "        num_question_answers (int): Number of question-answer pairs to generate\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: List of question-answer pairs\n",
    "    \"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"You are an expert on Retrieval-Augmented Generation (RAG) systems. Based on the provided contents from the document,\n",
    "    generate a set of question-answer pairs. The questions should be relevant to the content and cover various aspects including the main idea, reasoning, and image interpretation.\n",
    "    The questions should be strinctly based the content of the document. \n",
    "\n",
    "    Generate 10 question-answer pairs:\n",
    "    - 5 factual questions (one about hallucination in LLMs),\n",
    "    - 5 analytical questions (critical thinking),\n",
    "    Categorize also the question-answer pairs as \"factual\", \"analytical\".\n",
    "    Answers should be brief and to the point, and image questions should refer to the figures explicitly.\"\"\"\n",
    "\n",
    "    # Extract text and captions\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "\n",
    "    for page in doc:\n",
    "        full_text += page.get_text()\n",
    "        for block in page.get_text(\"dict\")[\"blocks\"]:\n",
    "            if block[\"type\"] == 0:\n",
    "                for line in block[\"lines\"]:\n",
    "                    line_text = \" \".join([span[\"text\"] for span in line[\"spans\"]])\n",
    "\n",
    "    doc.close()\n",
    "\n",
    "    user_prompt = f\"\"\"Document Text:\n",
    "    {full_text[:4000]}  # Truncated to first 4000 chars to fit prompt length\n",
    "\n",
    "    Generate {num_question_answers} question-answer pairs based on the above content.\n",
    "    \"\"\"\n",
    "   \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    # Extract the content from the response\n",
    "    content = response.choices[0].message.content\n",
    "    question_type = None\n",
    "    qa_list = []\n",
    "\n",
    "    sections = re.split(r\"### (Factual|Analytical) Questions\", content)\n",
    "\n",
    "    # Skip the initial empty or heading text\n",
    "    for i in range(1, len(sections), 2):\n",
    "        question_type = sections[i].strip()\n",
    "        section_text = sections[i + 1]\n",
    "\n",
    "        # Find question-answer pairs\n",
    "        qa_pairs = re.findall(\n",
    "            r\"\\*\\*Question:\\*\\*\\s*(.*?)\\n\\s*-\\s*\\*\\*Answer:\\*\\*\\s*(.*?)(?=\\n\\d+\\.|\\Z)\",\n",
    "            section_text,\n",
    "            flags=re.DOTALL\n",
    "        )\n",
    "\n",
    "        for question, answer in qa_pairs:\n",
    "            qa_list.append({\n",
    "                \"question\": question.strip(),\n",
    "                \"answer\": answer.strip(),\n",
    "                \"type\": question_type\n",
    "            })\n",
    "        \n",
    "    # Save the question-answer pairs to a JSON file\n",
    "    with open(\"data/val.json\", \"w\") as f:\n",
    "        json.dump(qa_list, f, indent=4)\n",
    "    return qa_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_responses(query, vector_response, graph_response, hybrid_response, reference_answer=None):\n",
    "    \"\"\"\n",
    "    Evaluate the responses from different retrieval methods.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_response (str): Response from vector-only RAG\n",
    "        graph_response (str): Response from graph-only RAG\n",
    "        hybrid_response (str): Response from hybrid approach RAG\n",
    "        reference_answer (str, optional): Reference answer\n",
    "        \n",
    "    Returns:\n",
    "        str: Evaluation of responses\n",
    "    \"\"\"\n",
    "    # System prompt for the evaluator to guide the evaluation process\n",
    "    system_prompt = \"\"\"You are an expert evaluator of RAG systems. Compare responses from three different retrieval approaches:\n",
    "    1. Vector-based retrieval: Uses semantic similarity for document retrieval\n",
    "    2. graph_response: Uses a knowledge graph to traverse and find relevant information\n",
    "    3. Hybrid approach: Combines both vector and graph methods\n",
    "\n",
    "\n",
    "    Evaluate the responses based on:\n",
    "    - Relevance to the query\n",
    "    - Factual correctness\n",
    "    - Comprehensiveness\n",
    "    - reasoning capability\n",
    "    - Clarity and coherence\"\"\"\n",
    "\n",
    "    # User prompt containing the query and responses\n",
    "    user_prompt = f\"\"\"Query: {query}\n",
    "\n",
    "    Vector-based response:\n",
    "    {vector_response}\n",
    "\n",
    "    graph-based response:\n",
    "    {graph_response}\n",
    "\n",
    "    Hybrid response:\n",
    "    {hybrid_response}\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Add reference answer to the prompt if provided\n",
    "    if reference_answer:\n",
    "        user_prompt += f\"\"\"\n",
    "            Reference answer:\n",
    "            {reference_answer}\n",
    "        \"\"\"\n",
    "\n",
    "    # Add instructions for detailed comparison to the user prompt\n",
    "    user_prompt += \"\"\"\n",
    "    Please provide a detailed comparison of these three responses. Which approach performed best for this query and why?\n",
    "    Be specific about the strengths and weaknesses of each approach for this particular query.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate the evaluation using meta-llama/Llama-3.2-3B-Instruct\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",  # Specify the model to use\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},  # System message to guide the evaluator\n",
    "            {\"role\": \"user\", \"content\": user_prompt}  # User message with query and responses\n",
    "        ],\n",
    "        temperature=0  # Set the temperature for response generation\n",
    "    )\n",
    "    \n",
    "    # Return the generated evaluation content\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_reference(response, reference, query):\n",
    "    \"\"\"\n",
    "    Compare generated response with reference answer.\n",
    "    \n",
    "    Args:\n",
    "        response (str): Generated response\n",
    "        reference (str): Reference answer\n",
    "        query (str): Original query\n",
    "        \n",
    "    Returns:\n",
    "        str: Comparison analysis\n",
    "    \"\"\"\n",
    "    # System message to instruct the model on how to compare the responses\n",
    "    system_message = \"\"\"Compare the AI-generated response with the reference answer.\n",
    "Evaluate based on: correctness, completeness, and relevance to the query.\n",
    "Provide a brief analysis (2-3 sentences) of how well the generated response matches the reference.\"\"\"\n",
    "\n",
    "    # Construct the prompt with the query, AI-generated response, and reference answer\n",
    "    prompt = f\"\"\"\n",
    "Query: {query}\n",
    "\n",
    "AI-generated response:\n",
    "{response}\n",
    "\n",
    "Reference answer:\n",
    "{reference}\n",
    "\n",
    "How well does the AI response match the reference?\n",
    "\"\"\"\n",
    "\n",
    "    # Make a request to the OpenAI API to generate the comparison analysis\n",
    "    comparison = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},  # System message to guide the assistant\n",
    "            {\"role\": \"user\", \"content\": prompt}  # User message with the prompt\n",
    "        ],\n",
    "        temperature=0.0  # Set the temperature for response generation\n",
    "    )\n",
    "    \n",
    "    # Return the generated comparison analysis\n",
    "    return comparison.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Graph RAG on a Sample PDF Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the PDF document containing AI information\n",
    "pdf_path = \"/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/Multimodal-Fusion-RAG/data/attention_is_all_you_need.pdf\"\n",
    "\n",
    "# Define an AI-related query for testing Graph RAG\n",
    "# query = \"What are the key applications of transformers in natural language processing?\"\n",
    "query = \"How does the encoding help to solve the lack of order of the input sequence?\"\n",
    "\n",
    "# Execute the Graph RAG pipeline to process the document and answer the query\n",
    "results = complete_rag_pipeline(pdf_path, query, chunk_size=1000, chunk_overlap=200, top_k=3, method=\"graph_only_rag\")\n",
    "\n",
    "# Print the response generated from the Graph RAG system\n",
    "print(\"\\n=== ANSWER ===\")\n",
    "print(results[\"response\"])\n",
    "\n",
    "# Define a test query and reference answer for formal evaluation\n",
    "# test_queries = [\n",
    "#     \"How do transformers handle sequential data compared to RNNs?\"\n",
    "# ]\n",
    "\n",
    "# # Reference answer for evaluation purposes\n",
    "# reference_answers = [\n",
    "#     \"Transformers handle sequential data differently from RNNs by using self-attention mechanisms instead of recurrent connections. This allows transformers to process all tokens in parallel rather than sequentially, capturing long-range dependencies more efficiently and enabling better parallelization during training. Unlike RNNs, transformers don't suffer from vanishing gradient problems with long sequences.\"\n",
    "# ]\n",
    "\n",
    "# # Run formal evaluation of the Graph RAG system with the test query\n",
    "# evaluation = evaluate_graph_rag(pdf_path, test_queries, reference_answers)\n",
    "\n",
    "# # Print evaluation summary statistics\n",
    "# print(\"\\n=== EVALUATION SUMMARY ===\")\n",
    "# print(f\"Graph nodes: {evaluation['graph_stats']['nodes']}\")\n",
    "# print(f\"Graph edges: {evaluation['graph_stats']['edges']}\")\n",
    "# for i, result in enumerate(evaluation['results']):\n",
    "#     print(f\"\\nQuery {i+1}: {result['query']}\")\n",
    "#     print(f\"Path length: {result['traversal_path_length']}\")\n",
    "#     print(f\"Chunks used: {result['relevant_chunks_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Evaluation Pipeline\n",
    "- For all the test query we generate a structured,comparative Evaluation results for 4 different retrieval strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hybrid_retrieval(pdf_path, test_queries, reference_answers=None, k=5):\n",
    "    \"\"\"\n",
    "    Evaluate hybrid retrieval compared to other methods.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        test_queries (List[str]): List of test queries\n",
    "        reference_answers (List[str], optional): Reference answers\n",
    "        k (int): Number of documents to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Evaluation results\n",
    "    \"\"\"\n",
    "    print(\"=== EVALUATING DIFFERENT RETRIEVAL STRATEGIES===\\n\")\n",
    "    \n",
    "    \n",
    "    # Initialize a list to store results for each query\n",
    "    results = []\n",
    "    \n",
    "    # Iterate over each test query\n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\n\\n=== Evaluating Query {i+1}/{len(test_queries)} ===\")\n",
    "        print(f\"Query: {query}\")\n",
    "        \n",
    "        # Get the reference answer if available\n",
    "        reference = None\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            reference = reference_answers[i]\n",
    "        \n",
    "        # Compare retrieval methods for the current query\n",
    "        comparison = compare_retrieval_methods(\n",
    "            query, \n",
    "            pdf_path=pdf_path, \n",
    "            k=k, \n",
    "            reference_answer=reference\n",
    "        )\n",
    "        \n",
    "        # Append the comparison results to the results list\n",
    "        results.append(comparison)\n",
    "        \n",
    "        # Print the responses from different retrieval methods\n",
    "        print(\"\\n=== Vector-based Response ===\")\n",
    "        print(comparison[\"vector_result\"][\"response\"])\n",
    "        \n",
    "        print(\"\\n=== Graph-based Response ===\")\n",
    "        print(comparison[\"graph_result\"][\"response\"])\n",
    "        \n",
    "        print(\"\\n=== Hybrid Response ===\")\n",
    "        print(comparison[\"hybrid_result\"][\"response\"])\n",
    "        \n",
    "        print(\"\\n=== Comparison ===\")\n",
    "        print(comparison[\"comparison\"])\n",
    "    \n",
    "    # Generate an overall analysis of the hybrid retrieval performance\n",
    "    overall_analysis = generate_overall_analysis(results)\n",
    "    \n",
    "    # Return the results and overall analysis\n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"overall_analysis\": overall_analysis\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_overall_analysis(results):\n",
    "    \"\"\"\n",
    "    Generate an overall analysis of hybrid retrieval.\n",
    "    \n",
    "    Args:\n",
    "        results (List[Dict]): Results from evaluating queries\n",
    "        \n",
    "    Returns:\n",
    "        str: Overall analysis\n",
    "    \"\"\"\n",
    "    # System prompt to guide the evaluation process\n",
    "    system_prompt = \"\"\"You are an expert at evaluating information retrieval systems. \n",
    "    Based on multiple test queries, provide an overall analysis comparing three retrieval approaches:\n",
    "    1. Vector-based retrieval (semantic similarity)\n",
    "    2. graph_response (knowledge graph traversal)\n",
    "    3. Hybrid approach (combination of vector and graph methods)\n",
    "\n",
    "    Focus on:\n",
    "    1. Types of queries where each approach performs best\n",
    "    2. Overall strengths and weaknesses of each approach\n",
    "    3. Is graph_response significantly better than vector-based retrieval?\n",
    "    4. When is graph_response advantageous over vector-based retrieval?\n",
    "    5. How hybrid retrieval combining both vector-based and graph-based balances the trade-offs\n",
    "    6. Recommendations for when to use each approach\n",
    "    \n",
    "    At the end generate a summary of the analysis with approximate score for each method in a tabular format in overall \n",
    "    which retrieval approaches is the best for most of questions with the score for following dimentions with generic \n",
    "    qualitative observation with column names as follows.\n",
    "\n",
    "    | Retrieval Method | Relevance | Factual Correctness | Reasoning Capability | Clarity and Coherence | Overall Score | observation\n",
    "    provide a higher score for the method which is best for most of the questions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a summary of evaluations for each query\n",
    "    evaluations_summary = \"\"\n",
    "    for i, result in enumerate(results):\n",
    "        evaluations_summary += f\"Query {i+1}: {result['query']}\\n\"\n",
    "        evaluations_summary += f\"Comparison Summary: {result['comparison'][:200]}...\\n\\n\"\n",
    "\n",
    "    # User prompt containing the evaluations summary\n",
    "    user_prompt = f\"\"\"Based on the following evaluations of different retrieval methods across {len(results)} queries, \n",
    "    provide an overall analysis comparing these three approaches:\n",
    "\n",
    "    {evaluations_summary}\n",
    "\n",
    "    Please provide a comprehensive analysis of vector-based, and graph retrieval approaches,\n",
    "    highlighting when and why graph retrieval provides advantages over the vector-store-based retrieval method.\"\"\"\n",
    "\n",
    "    # Generate the overall analysis using meta-llama/Llama-3.2-3B-Instruct\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Return the generated analysis content\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Hybrid Retrieval\n",
    "\n",
    "- Running the evaluator for all the reference queries and respective answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to validation document with questions and answers\n",
    "\n",
    "# Path to the PDF document to be evaluated\n",
    "# pdf_path = \"data/RAG_white_papers_articles.pdf\"\n",
    "pdf_path = \"/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/Multimodal-Fusion-RAG/data/RAG_white_papers_articles.pdf\"\n",
    "val_path = \"data/val.json\"\n",
    "# generate_validation_questions_anaswers(pdf_path, num_question_answers=10)\n",
    "validation_doc = json.load(open(val_path, \"r\"))\n",
    "\n",
    "# test_queries = [item['question'] for item in validation_doc]\n",
    "\n",
    "## Quick test queries\n",
    "test_queries = [\n",
    "    \n",
    "    # \"How does the encoding help to solve the lack of order of the input sequence?\",\n",
    "    # \"Why might Modular RAG offer a significant advantage over Naive or Advanced RAG in real-world applications?\",\n",
    "    # \"What is a primary challenge that LLMs face which RAG is designed to solve?\"\n",
    "]\n",
    "\n",
    "# Optional reference answer\n",
    "\n",
    "# reference_answers = [item['answer'] for item in validation_doc]\n",
    "\n",
    "reference_answers = [\n",
    "    #  \"This is done by adding positional encodings to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings in the Transformer model is to inject information about the relative or absolute position of the tokens in the sequence. The positional encodings have the same dimensionality as the embeddings, so they can be summed together. By using sine and cosine functions of different frequencies, the positional encodings form a geometric progression from 2π to 10000 · 2π. This allows the model to easily learn to attend by relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of PEpos.\",\n",
    "    # \"Modular RAG provides flexibility by allowing modules to be rearranged or replaced, enabling better adaptation to diverse tasks, reducing redundancy, and supporting more dynamic interaction flows\",\n",
    "    # \"Hallucination—producing content not grounded in factual sources—is a key challenge RAG addresses by grounding generation in retrieved external knowledge\"\n",
    "\n",
    "]\n",
    "k = 4\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results = evaluate_hybrid_retrieval(\n",
    "    pdf_path=pdf_path,\n",
    "    test_queries=test_queries,\n",
    "    reference_answers=reference_answers,\n",
    "    k=k,\n",
    ")\n",
    "\n",
    "# Print overall analysis\n",
    "print(\"\\n\\n=== OVERALL ANALYSIS ===\\n\")\n",
    "print(evaluation_results[\"overall_analysis\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
