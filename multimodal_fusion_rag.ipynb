{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Multimodal Fusion Retrieval: Combining Multimodal Vector-store and Keyword Search\n",
    "\n",
    "In this notebook, we implement a fusion retrieval system that combines the strengths of both\n",
    "\n",
    "**A) semantic vector search - Enhanced with multimodal context retrieval(retrieval based on both text and images)** \n",
    "\n",
    "**B) keyword-based BM25 retrieval.(https://en.wikipedia.org/wiki/Okapi_BM25)**\n",
    "\n",
    "This approach improves retrieval quality by capturing both conceptual similarity and exact keyword matches.\n",
    "\n",
    "## Why Fusion Retrieval Matters\n",
    "\n",
    "Traditional RAG systems(https://en.wikipedia.org/wiki/Retrieval-augmented_generation) typically rely on vector search alone mostly based on texts, but this has limitations:\n",
    "\n",
    "- Vector search may miss the essential relevant information from images \n",
    "- Vector search excels at semantic similarity but may miss exact keyword matches\n",
    "- Keyword search is great for specific terms but lacks semantic understanding\n",
    "- Different queries perform better with different retrieval methods - may both rely on texts and images or in combination with keyword matches\n",
    "\n",
    "Multimodal Fusion retrieval gives us the best of both worlds by:\n",
    "\n",
    "- Performing both vector-based and keyword-based retrieval\n",
    "- Enhances retrieval quality by augmenting with image captions \n",
    "- Normalizing the scores from each approach\n",
    "- Combining them with a weighted formula\n",
    "- Ranking documents based on the combined score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "We begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "import fitz\n",
    "import io\n",
    "from PIL import Image\n",
    "from openai import OpenAI\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import tempfile\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import base64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the OpenAI API Client\n",
    "We initialize the OpenAI client to generate embeddings and responses.\n",
    "\n",
    "- An important step is to get an OPENAI_API_KEY from https://platform.openai.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client with the base URL and API key\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.openai.com/v1/\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # Retrieve the API key from environment variables\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing Functions\n",
    "\n",
    "- Basic cleaning of text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text by removing extra whitespace and special characters.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    # Replace multiple whitespace characters (including newlines and tabs) with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Fix common OCR issues by replacing tab and newline characters with a space\n",
    "    text = text.replace('\\\\t', ' ')\n",
    "    text = text.replace('\\\\n', ' ')\n",
    "    \n",
    "    # Remove any leading or trailing whitespace and ensure single spaces between words\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multimodal content extraction pipeline\n",
    "\n",
    "- Content extraction pagewise\n",
    "- We have option whether to extract image information or not\n",
    "- Segregating texts and image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content_from_pdf(pdf_path, use_image = True, output_dir=None):\n",
    "    \"\"\"\n",
    "    Extract both text and images from a PDF file.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        output_dir (str, optional): Directory to save extracted images\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[List[Dict], List[Dict]]: Text data and image data\n",
    "    \"\"\"\n",
    "    # Create a temporary directory for images if not provided\n",
    "    temp_dir = None\n",
    "    if output_dir is None:\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "        output_dir = temp_dir\n",
    "    else:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "    text_data = []  # List to store extracted text data\n",
    "    image_paths = []  # List to store paths of extracted images\n",
    "    # text_only = \"\"\n",
    "    print(f\"Extracting content from {pdf_path}...\")\n",
    "    \n",
    "    try:\n",
    "        with fitz.open(pdf_path) as pdf_file:\n",
    "            # Loop through every page in the PDF\n",
    "            for page_number in range(len(pdf_file)):\n",
    "                page = pdf_file[page_number]\n",
    "                \n",
    "                # Extract text from the page\n",
    "                text = page.get_text().strip()\n",
    "                # text_only += text\n",
    "                if text:\n",
    "                    text_data.append({\n",
    "                        \"content\": text,\n",
    "                        \"metadata\": {\n",
    "                            \"source\": pdf_path,\n",
    "                            \"page\": page_number + 1,\n",
    "                            \"type\": \"text\"\n",
    "                        }\n",
    "                    })\n",
    "                \n",
    "                # Extract images from the page\n",
    "                if not use_image:\n",
    "                    continue\n",
    "                image_list = page.get_images(full=True)\n",
    "                for img_index, img in enumerate(image_list):\n",
    "                    xref = img[0]  # XREF of the image\n",
    "                    base_image = pdf_file.extract_image(xref)\n",
    "                    \n",
    "                    if base_image:\n",
    "                        image_bytes = base_image[\"image\"]\n",
    "                        image_ext = base_image[\"ext\"]\n",
    "                        \n",
    "                        # Save the image to the output directory\n",
    "                        img_filename = f\"page_{page_number+1}_img_{img_index+1}.{image_ext}\"\n",
    "                        img_path = os.path.join(output_dir, img_filename)\n",
    "                        \n",
    "                        with open(img_path, \"wb\") as img_file:\n",
    "                            img_file.write(image_bytes)\n",
    "                        \n",
    "                        image_paths.append({\n",
    "                            \"path\": img_path,\n",
    "                            \"metadata\": {\n",
    "                                \"source\": pdf_path,\n",
    "                                \"page\": page_number + 1,\n",
    "                                \"image_index\": img_index + 1,\n",
    "                                \"type\": \"image\"\n",
    "                            }\n",
    "                        })\n",
    "  \n",
    "        print(f\"Extracted {len(text_data)} text segments and {len(image_paths)} images\")\n",
    "        return text_data, image_paths\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting content: {e}\")\n",
    "        if temp_dir and os.path.exists(temp_dir):\n",
    "            shutil.rmtree(temp_dir)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking the Extracted Text\n",
    "Once we have the extracted text, we divide it into smaller, overlapping chunks - more manageable pieces to improve retrieval accuracy and reduce computational overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text_data, chunk_size=1000, overlap=200):\n",
    "    \"\"\"\n",
    "    Split text data into overlapping chunks.\n",
    "    \n",
    "    Args:\n",
    "        text_data (List[Dict]): Text data extracted from PDF\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        overlap (int): Overlap between chunks in characters\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Chunked text data\n",
    "    \"\"\"\n",
    "    chunked_data = []  # Initialize an empty list to store chunked data\n",
    "    \n",
    "    for item in text_data:\n",
    "        text = item[\"content\"]  # Extract the text content\n",
    "        text = clean_text(text)\n",
    "        metadata = item[\"metadata\"]  # Extract the metadata\n",
    "        \n",
    "        # Skip if text is too short\n",
    "        if len(text) < chunk_size / 2:\n",
    "            chunked_data.append({\n",
    "                \"content\": text,\n",
    "                \"metadata\": metadata\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Create chunks with overlap\n",
    "        chunks = []\n",
    "        for i in range(0, len(text), chunk_size - overlap):\n",
    "            chunk = text[i:i + chunk_size]  # Extract a chunk of the specified size\n",
    "            if chunk:  # Ensure we don't add empty chunks\n",
    "                chunks.append(chunk)\n",
    "        \n",
    "        # Add each chunk with updated metadata\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_metadata = metadata.copy()  # Copy the original metadata\n",
    "            chunk_metadata[\"chunk_index\"] = i  # Add chunk index to metadata\n",
    "            chunk_metadata[\"chunk_count\"] = len(chunks)  # Add total chunk count to metadata\n",
    "            \n",
    "            chunked_data.append({\n",
    "                \"content\": chunk,  # The chunk text\n",
    "                \"metadata\": chunk_metadata  # The updated metadata\n",
    "            })\n",
    "    \n",
    "    print(f\"Created {len(chunked_data)} text chunks\")  # Print the number of created chunks\n",
    "    return chunked_data  # Return the list of chunked data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Processing Functions\n",
    "\n",
    "- Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    \"\"\"\n",
    "    Encode an image file as base64.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        \n",
    "    Returns:\n",
    "        str: Base64 encoded image\n",
    "    \"\"\"\n",
    "    # Open the image file in binary read mode\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        # Read the image file and encode it to base64\n",
    "        encoded_image = base64.b64encode(image_file.read())\n",
    "        # Decode the base64 bytes to a string and return\n",
    "        return encoded_image.decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image caption generation\n",
    "\n",
    "- We use here generic gpt-4o model, however it's worth trying other specialized models like **llava-1.5-7b**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_caption(image_path):\n",
    "    \"\"\"\n",
    "    Generate a caption for an image using OpenAI's vision capabilities.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated caption\n",
    "    \"\"\"\n",
    "    # Check if the file exists and is an image\n",
    "    if not os.path.exists(image_path):\n",
    "        return \"Error: Image file not found\"\n",
    "    \n",
    "    try:\n",
    "        # Open and validate the image\n",
    "        Image.open(image_path)\n",
    "        \n",
    "        # Encode the image to base64\n",
    "        base64_image = encode_image(image_path)\n",
    "        \n",
    "        # Create the API request to generate the caption\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\", # Use the llava-1.5-7b model\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an assistant specialized in describing images from academic papers. \"\n",
    "                    \"Provide detailed captions for the image that capture key information. \"\n",
    "                    \"If the image contains charts, tables, or diagrams, describe their content and purpose clearly. \"\n",
    "                    \"Your caption should be optimized for future retrieval when people ask questions about this content.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"Describe this image in detail, focusing on its academic content:\"},\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=300\n",
    "        )\n",
    "        \n",
    "        # Extract the caption from the response\n",
    "        caption = response.choices[0].message.content\n",
    "        return caption\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Return an error message if an exception occurs\n",
    "        return f\"Error generating caption: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Image data creation with metadata(reference, page no etc. of the image) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(image_paths):\n",
    "    \"\"\"\n",
    "    Process all images and generate captions.\n",
    "    \n",
    "    Args:\n",
    "        image_paths (List[Dict]): Paths to extracted images\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Image data with captions\n",
    "    \"\"\"\n",
    "    image_data = []  # Initialize an empty list to store image data with captions\n",
    "    \n",
    "    print(f\"Generating captions for {len(image_paths)} images...\")  # Print the number of images to process\n",
    "    for i, img_item in enumerate(image_paths):\n",
    "        print(f\"Processing image {i+1}/{len(image_paths)}...\")  # Print the current image being processed\n",
    "        img_path = img_item[\"path\"]  # Get the image path\n",
    "        metadata = img_item[\"metadata\"]  # Get the image metadata\n",
    "        \n",
    "        # Generate caption for the image\n",
    "        caption = generate_image_caption(img_path)\n",
    "        \n",
    "        # Add the image data with caption to the list\n",
    "        image_data.append({\n",
    "            \"content\": caption,  # The generated caption\n",
    "            \"metadata\": metadata,  # The image metadata\n",
    "            \"image_path\": img_path  # The path to the image\n",
    "        })\n",
    "    \n",
    "    return image_data  # Return the list of image data with captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings for Text Chunks\n",
    "Embeddings transform text into numerical vectors, which allow for efficient similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(texts, model=\"text-embedding-3-large\"):\n",
    "    \"\"\"\n",
    "    Create embeddings for the given texts.\n",
    "    \n",
    "    Args:\n",
    "        texts (str or List[str]): Input text(s)\n",
    "        model (str): Embedding model name\n",
    "        \n",
    "    Returns:\n",
    "        List[List[float]]: Embedding vectors\n",
    "    \"\"\"\n",
    "    # Handle both string and list inputs\n",
    "    input_texts = texts if isinstance(texts, list) else [texts]\n",
    "    \n",
    "    # Process in batches if needed (OpenAI API limits)\n",
    "    batch_size = 100\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # Iterate over the input texts in batches\n",
    "    for i in range(0, len(input_texts), batch_size):\n",
    "        batch = input_texts[i:i + batch_size]  # Get the current batch of texts\n",
    "        \n",
    "        # Create embeddings for the current batch\n",
    "        response = client.embeddings.create(\n",
    "            model=model,\n",
    "            input=batch\n",
    "        )\n",
    "        \n",
    "        # Extract embeddings from the response\n",
    "        batch_embeddings = [item.embedding for item in response.data]\n",
    "        all_embeddings.extend(batch_embeddings)  # Add the batch embeddings to the list\n",
    "    \n",
    "    # If input was a string, return just the first embedding\n",
    "    if isinstance(texts, str):\n",
    "        return all_embeddings[0]\n",
    "    \n",
    "    # Otherwise return all embeddings\n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Multimodal Vector Store\n",
    "\n",
    "- A simplified real time object based vector store implementation. An industry level application may require a permanent vector database consisting embeddings and lookup tables of entire knowledge base for faster and efficient retrieval\n",
    "\n",
    "- The VectorStore object is consist of\n",
    "   - chunk embeddings\n",
    "   - contents/texts\n",
    "   - metadata of elements\n",
    "\n",
    "\n",
    "- The VectorStore object has `similarity_search` recipe of finding the most similar items to a query embedding based on **cosine similarity** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalVectorStore:\n",
    "    \"\"\"\n",
    "    A simple vector store implementation for multi-modal content.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Initialize lists to store vectors, contents, and metadata\n",
    "        self.vectors = []\n",
    "        self.contents = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_item(self, content, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        Add an item to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            content (str): The content (text or image caption)\n",
    "            embedding (List[float]): The embedding vector\n",
    "            metadata (Dict, optional): Additional metadata\n",
    "        \"\"\"\n",
    "        # Append the embedding vector, content, and metadata to their respective lists\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.contents.append(content)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def add_items(self, items, embeddings):\n",
    "        \"\"\"\n",
    "        Add multiple items to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            items (List[Dict]): List of content items\n",
    "            embeddings (List[List[float]]): List of embedding vectors\n",
    "        \"\"\"\n",
    "        # Loop through items and embeddings and add each to the vector store\n",
    "        for i, (item, embedding) in enumerate(zip(items, embeddings)):\n",
    "            self.add_item(\n",
    "                # text=item[\"content\"],  # Extract text from item\n",
    "                content=item[\"content\"],\n",
    "                embedding=embedding,\n",
    "                metadata={**item.get(\"metadata\", {}), \"index\": i}\n",
    "            )\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        Find the most similar items to a query embedding.\n",
    "        \n",
    "        Args:\n",
    "            query_embedding (List[float]): Query embedding vector\n",
    "            k (int): Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: Top k most similar items\n",
    "        \"\"\"\n",
    "        # Return an empty list if there are no vectors in the store\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        \n",
    "        # Convert query embedding to numpy array\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # Calculate similarities using cosine similarity\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top k results\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                # \"text\": self.contents[idx],  # Retrieve text by index\n",
    "                \"content\": self.contents[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": float(score)  # Convert to float for JSON serialization\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_all_documents(self):\n",
    "        \"\"\"\n",
    "        Get all documents in the store.\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict]: All documents\n",
    "        \"\"\"\n",
    "        return [{\"text\": content, \"metadata\": meta} for content, meta in zip(self.contents, self.metadata)]  # Combine texts and metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25 Implementation\n",
    "\n",
    "- BM25, or Best Match 25, also known as Okapi BM25, is a ranking algorithm for information retrieval and search engines that determines a document's relevance to a given query and ranks documents based on their relevance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bm25_index(chunks):\n",
    "    \"\"\"\n",
    "    Create a BM25 index from the given chunks.\n",
    "    \n",
    "    Args:\n",
    "        chunks (List[Dict]): List of text chunks\n",
    "        \n",
    "    Returns:\n",
    "        BM25Okapi: A BM25 index\n",
    "    \"\"\"\n",
    "    # Extract text from each chunk\n",
    "    texts = [chunk[\"content\"] for chunk in chunks]\n",
    "    \n",
    "    # Tokenize each document by splitting on whitespace\n",
    "    tokenized_docs = [text.split() for text in texts]\n",
    "    \n",
    "    # Create the BM25 index using the tokenized documents\n",
    "    bm25 = BM25Okapi(tokenized_docs)\n",
    "    \n",
    "    # Print the number of documents in the BM25 index\n",
    "    print(f\"Created BM25 index with {len(texts)} documents\")\n",
    "    \n",
    "    return bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Based on the BM25 index, chunked texts and a given search query retrieves the top k context documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_search(bm25, chunks, query, k=5):\n",
    "    \"\"\"\n",
    "    Search the BM25 index with a query.\n",
    "    \n",
    "    Args:\n",
    "        bm25 (BM25Okapi): BM25 index\n",
    "        chunks (List[Dict]): List of text chunks\n",
    "        query (str): Query string\n",
    "        k (int): Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Top k results with scores\n",
    "    \"\"\"\n",
    "    # Tokenize the query by splitting it into individual words\n",
    "    query_tokens = query.split()\n",
    "    \n",
    "    # Get BM25 scores for the query tokens against the indexed documents\n",
    "    scores = bm25.get_scores(query_tokens)\n",
    "    \n",
    "    # Initialize an empty list to store results with their scores\n",
    "    results = []\n",
    "    \n",
    "    # Iterate over the scores and corresponding chunks\n",
    "    for i, score in enumerate(scores):\n",
    "        # Create a copy of the metadata to avoid modifying the original\n",
    "        metadata = chunks[i].get(\"metadata\", {}).copy()\n",
    "        # Add index to metadata\n",
    "        metadata[\"index\"] = i\n",
    "        \n",
    "        results.append({\n",
    "            \"text\": chunks[i][\"content\"],\n",
    "            \"metadata\": metadata,  # Add metadata with index\n",
    "            \"bm25_score\": float(score)\n",
    "        })\n",
    "    \n",
    "    # Sort the results by BM25 score in descending order\n",
    "    results.sort(key=lambda x: x[\"bm25_score\"], reverse=True)\n",
    "    \n",
    "    # Return the top k results\n",
    "    return results[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fusion Retrieval Function\n",
    "\n",
    "- Finally combining merits of both multimodal vector_store results and BM25 search based results\n",
    "- calculates the `combined_score` based on `alpha` - weight distribution between vector_store based score and BM25 search based score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fusion_retrieval(query, chunks, vector_store, bm25_index, k=5, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Perform fusion retrieval combining multimodal vector-based and BM25 search.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Query string\n",
    "        chunks (List[Dict]): Original text chunks\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        bm25_index (BM25Okapi): BM25 index\n",
    "        k (int): Number of results to return\n",
    "        alpha (float): Weight for vector scores (0-1), where 1-alpha is BM25 weight\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Top k results based on combined scores\n",
    "    \"\"\"\n",
    "    print(f\"Performing fusion retrieval for query: {query}\")\n",
    "    \n",
    "    # Define small epsilon to avoid division by zero\n",
    "    epsilon = 1e-8\n",
    "    \n",
    "    # Get vector search results\n",
    "    query_embedding = create_embeddings(query)  # Create embedding for the query\n",
    "    vector_results = vector_store.similarity_search(query_embedding, k=len(chunks))  # Perform vector search\n",
    "    \n",
    "    # Separate text and image results\n",
    "    text_results = [r for r in vector_results if r[\"metadata\"].get(\"type\") == \"text\"]\n",
    "    image_results = [r for r in vector_results if r[\"metadata\"].get(\"type\") == \"image\"]\n",
    "    \n",
    "    print(f\"Retrieved {len(vector_results)} relevant items ({len(text_results)} text, {len(image_results)} image captions)\")\n",
    "\n",
    "    # Get BM25 search results\n",
    "    bm25_results = bm25_search(bm25_index, chunks, query, k=len(chunks))  # Perform BM25 search\n",
    "    \n",
    "    # Create dictionaries to map document index to score\n",
    "    vector_scores_dict = {result[\"metadata\"][\"index\"]: result[\"similarity\"] for result in vector_results}\n",
    "    bm25_scores_dict = {result[\"metadata\"][\"index\"]: result[\"bm25_score\"] for result in bm25_results}\n",
    "    \n",
    "    # Ensure all documents have scores for both methods\n",
    "    all_docs = vector_store.get_all_documents()\n",
    "    combined_results = []\n",
    "    \n",
    "    for i, doc in enumerate(all_docs):\n",
    "        vector_score = vector_scores_dict.get(i, 0.0)  # Get vector score or 0 if not found\n",
    "        bm25_score = bm25_scores_dict.get(i, 0.0)  # Get BM25 score or 0 if not found\n",
    "        combined_results.append({\n",
    "            \"text\": doc[\"text\"],\n",
    "            \"metadata\": doc[\"metadata\"],\n",
    "            \"vector_score\": vector_score,\n",
    "            \"bm25_score\": bm25_score,\n",
    "            \"index\": i\n",
    "        })\n",
    "    \n",
    "    # Extract scores as arrays\n",
    "    vector_scores = np.array([doc[\"vector_score\"] for doc in combined_results])\n",
    "    bm25_scores = np.array([doc[\"bm25_score\"] for doc in combined_results])\n",
    "    \n",
    "    # Normalize scores\n",
    "    norm_vector_scores = (vector_scores - np.min(vector_scores)) / (np.max(vector_scores) - np.min(vector_scores) + epsilon)\n",
    "    norm_bm25_scores = (bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) - np.min(bm25_scores) + epsilon)\n",
    "    \n",
    "    # Compute combined scores\n",
    "    combined_scores = alpha * norm_vector_scores + (1 - alpha) * norm_bm25_scores\n",
    "    \n",
    "    # Add combined scores to results\n",
    "    for i, score in enumerate(combined_scores):\n",
    "        combined_results[i][\"combined_score\"] = float(score)\n",
    "    \n",
    "    # Sort by combined score (descending)\n",
    "    combined_results.sort(key=lambda x: x[\"combined_score\"], reverse=True)\n",
    "    \n",
    "    # Return top k results\n",
    "    top_results = combined_results[:k]\n",
    "    \n",
    "    print(f\"Retrieved {len(top_results)} documents with fusion retrieval\")\n",
    "    return top_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing Pipeline\n",
    "\n",
    "- We have option whether to use image information or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, use_image = True, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Process a document for fusion retrieval.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        chunk_overlap (int): Overlap between chunks in characters\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[List[Dict], SimpleVectorStore, BM25Okapi]: Chunks, vector store, and BM25 index\n",
    "    \"\"\"\n",
    "    \n",
    "    image_dir = \"extracted_images\"\n",
    "    os.makedirs(image_dir, exist_ok=True)\n",
    "    # Extract text and images from the PDF\n",
    "    if use_image:\n",
    "        text, image_paths = extract_content_from_pdf(pdf_path, image_dir)\n",
    "    \n",
    "    else:\n",
    "        text, image_paths = extract_content_from_pdf(pdf_path, use_image=False)\n",
    "\n",
    "    # process the extracted images to generate captions\n",
    "    image_data = process_images(image_paths)        \n",
    "    \n",
    "    # Split the cleaned text into overlapping chunks\n",
    "    chunked_text = chunk_text(text, chunk_size, chunk_overlap)\n",
    "    \n",
    "    # Combine all content items (text chunks and image captions)\n",
    "    all_items = chunked_text + image_data\n",
    "    \n",
    "    # Extract content for embedding\n",
    "    contents = [item[\"content\"] for item in all_items]\n",
    "    \n",
    "    # Create embeddings for all content\n",
    "    print(\"Creating embeddings for all content...\")\n",
    "    embeddings = create_embeddings(contents)\n",
    "    \n",
    "    # Build the vector store and add items with their embeddings\n",
    "    vector_store = MultiModalVectorStore()\n",
    "    vector_store.add_items(all_items, embeddings)\n",
    "\n",
    "    # Prepare document info with counts of text chunks and image captions\n",
    "    doc_info = {\n",
    "        \"text_count\": len(chunked_text),\n",
    "        \"image_count\": len(image_data),\n",
    "        \"total_items\": len(all_items),\n",
    "    }\n",
    "    \n",
    "    # Print summary of added items\n",
    "    print(f\"Added {len(all_items)} items to vector store ({len(chunked_text)} text chunks, {len(image_data)} image captions)\")\n",
    "    \n",
    "    # Create a BM25 index from the chunks\n",
    "    bm25_index = create_bm25_index(chunked_text)\n",
    "    \n",
    "    # Return the chunks, vector store, and BM25 index\n",
    "    return chunked_text, vector_store, bm25_index, doc_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation\n",
    "\n",
    "- Based on the query and retrieved results as context generate the final answer.\n",
    "- gpt-4o is used as LLM brain, however worth exploring _Meta’s Llama family models_\n",
    "- Also temperature plays crucial role in response generation, the degree of exploration(randomness) or exploitation(deterministic).I used a moderate temperature = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, results):\n",
    "    \"\"\"\n",
    "    Generate a response based on the query and retrieved results.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        results (List[Dict]): Retrieved content\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    # Format the context from the retrieved results\n",
    "    context = \"\"\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        print(result)\n",
    "        # Determine the type of content (text or image caption)\n",
    "        content_type = \"Text\" if result[\"metadata\"].get(\"type\") == \"text\" else \"Image caption\"\n",
    "        # Get the page number from the metadata\n",
    "        page_num = result[\"metadata\"].get(\"page\", \"unknown\")\n",
    "        \n",
    "        # Append the content type and page number to the context\n",
    "        context += f\"[{content_type} from page {page_num}]\\n\"\n",
    "        # Append the actual content to the context\n",
    "        context += result[\"text\"]\n",
    "        context += \"\\n\\n\"\n",
    "    \n",
    "    # System message to guide the AI assistant\n",
    "    system_message = \"\"\"You are an AI assistant specializing in answering questions about documents \n",
    "    that contain both text and images. You have been given relevant text passages and image captions \n",
    "    from the document. Use this information to provide a comprehensive, accurate response to the query.\n",
    "    If information comes from an image or chart, mention this in your answer.\n",
    "    If the retrieved information doesn't fully answer the query, acknowledge the limitations.\"\"\"\n",
    "\n",
    "    # User message containing the query and the formatted context\n",
    "    user_message = f\"\"\"Query: {query}\n",
    "\n",
    "    Retrieved content:\n",
    "    {context}\n",
    "\n",
    "    Please answer the query based on the retrieved content.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the response using the OpenAI API\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    # Return the generated response\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Retrieval Function\n",
    "\n",
    "- Combining multimodal vector_store and BM25 based search returns the final response and retrieved documents for user review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Retrieval Methods\n",
    "\n",
    "### I compared 4 retrieval methods to assess pros and cons of each\n",
    "\n",
    "- `build_text_only_store` - text-only vector store and retrieval with out bm25 method\n",
    "\n",
    "- `build_text_image_store` - text and image based multimodal vector store and retrieval with out bm25 method\n",
    "\n",
    "- `bm25_only_rag` - based on only bm25 based search result\n",
    "\n",
    "- `answer_with_multimodal_fusion_rag` - fusion based rag, combining multimodal vector store and bm25 based search and scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_text_only_store(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Build a text-only vector store for comparison.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        chunk_overlap (int): Overlap between chunks in characters\n",
    "        \n",
    "    Returns:\n",
    "        VectorStore: Text-only vector store\n",
    "    \"\"\"\n",
    "    # Extract text from PDF (reuse function but ignore images)\n",
    "    text_data, _ = extract_content_from_pdf(pdf_path, False, None)\n",
    "    \n",
    "    # Chunk text\n",
    "    chunked_text = chunk_text(text_data, chunk_size, chunk_overlap)\n",
    "    \n",
    "    # Extract content for embedding\n",
    "    contents = [item[\"content\"] for item in chunked_text]\n",
    "    \n",
    "    # Create embeddings\n",
    "    print(\"Creating embeddings for text-only content...\")\n",
    "    embeddings = create_embeddings(contents)\n",
    "    \n",
    "    # Build vector store\n",
    "    vector_store = MultiModalVectorStore()\n",
    "    vector_store.add_items(chunked_text, embeddings)\n",
    "    \n",
    "    print(f\"Added {len(chunked_text)} text items to text-only vector store\")\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_text_image_store(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Build a text-only vector store for comparison.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        chunk_overlap (int): Overlap between chunks in characters\n",
    "        \n",
    "    Returns:\n",
    "        MultiModalVectorStore: Text-only vector store\n",
    "    \"\"\"\n",
    "    # Extract text from PDF (reuse function but ignore images)\n",
    "    text_data, _ = extract_content_from_pdf(pdf_path, None)\n",
    "    \n",
    "    # Chunk text\n",
    "    chunked_text = chunk_text(text_data, chunk_size, chunk_overlap)\n",
    "    \n",
    "    # Extract content for embedding\n",
    "    contents = [item[\"content\"] for item in chunked_text]\n",
    "    \n",
    "    # Create embeddings\n",
    "    print(\"Creating embeddings for text-only content...\")\n",
    "    embeddings = create_embeddings(contents)\n",
    "    \n",
    "    # Build vector store\n",
    "    vector_store = MultiModalVectorStore()\n",
    "    vector_store.add_items(chunked_text, embeddings)\n",
    "    \n",
    "    print(f\"Added {len(chunked_text)} text items to text-image vector store\")\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_only_rag(query, chunks, bm25_index, k=5):\n",
    "    \"\"\"\n",
    "    Answer a query using only BM25-based RAG.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        chunks (List[Dict]): Text chunks\n",
    "        bm25_index (BM25Okapi): BM25 index\n",
    "        k (int): Number of documents to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Query results\n",
    "    \"\"\"\n",
    "    # Retrieve documents using BM25 search\n",
    "    retrieved_docs = bm25_search(bm25_index, chunks, query, k=k)\n",
    "    \n",
    "    # Format the context from the retrieved documents by joining their text with separators\n",
    "    # context = \"\\n\\n---\\n\\n\".join([doc[\"text\"] for doc in retrieved_docs])\n",
    "    \n",
    "    # Generate a response based on the query and the formatted context\n",
    "    response = generate_response(query, retrieved_docs)\n",
    "    \n",
    "    # Return the query, retrieved documents, and the generated response\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"retrieved_documents\": retrieved_docs,\n",
    "        \"response\": response\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_multimodal_fusion_rag(query, chunks, vector_store, bm25_index, k=5, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Answer a query using fusion RAG.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        chunks (List[Dict]): Text chunks\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        bm25_index (BM25Okapi): BM25 index\n",
    "        k (int): Number of documents to retrieve\n",
    "        alpha (float): Weight for vector scores\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Query results including retrieved documents and response\n",
    "    \"\"\"\n",
    "    # Retrieve documents using fusion retrieval method\n",
    "    retrieved_docs = fusion_retrieval(query, chunks, vector_store, bm25_index, k=k, alpha=alpha)\n",
    "    \n",
    "    # Generate a response based on the query and the formatted context\n",
    "    response = generate_response(query, retrieved_docs)\n",
    "    \n",
    "    # Return the query, retrieved documents, and the generated response\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"retrieved_documents\": retrieved_docs,\n",
    "        \"response\": response\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Method to generate response and k nos of retrieved documents given  a query and vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_only_rag(query, vector_store, k=5):\n",
    "    \"\"\"\n",
    "    Answer a query using only vector-based RAG.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        k (int): Number of documents to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Query results\n",
    "    \"\"\"\n",
    "    # Create query embedding\n",
    "    query_embedding = create_embeddings(query)\n",
    "    \n",
    "    # Retrieve documents using vector-based similarity search\n",
    "    retrieved_docs = vector_store.similarity_search(query_embedding, k=k)\n",
    "    print(f\"Retrieved {len(retrieved_docs)} documents using vector search\")\n",
    "    for doc in retrieved_docs:\n",
    "        doc['text'] = doc.pop('content')\n",
    "    \n",
    "    \n",
    "    # Generate a response based on the query and the formatted context\n",
    "    response = generate_response(query, retrieved_docs)\n",
    "    \n",
    "    # Return the query, retrieved documents, and the generated response\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"retrieved_documents\": retrieved_docs,\n",
    "        \"response\": response\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluates merits of above mentioned 4 retrieval strategies.\n",
    "\n",
    "- There are different industry standard statistical metrics in NLP - BLEU, ROUGE, MRR, BERTScore etc. However in LLM world a qualitative evaluation is more relevant\n",
    "- A manual review of all the the generated answers and retrieved documents would be too time consuming and also error prone\n",
    "- I created the following frame work as qualitative evaluation\n",
    "  \n",
    "  -  __LLM as Generator__: Replace human effort by generating ___n___ nos of question-answer pairs by LLM using structured prompts from the pdf document as validation data\n",
    "  \n",
    "  (** it's possible that the system generate different sets of validation question-answer pairs each time the generator functions run.This is actually the beauty of the system and make the validation system random to navigate possible memorization)\n",
    "  \n",
    "  -  __LLM as Evaluator__: Replace human effort of qualitative evaluation by evaluating the question-answer pairs by LLM itself using well designed prompts.Further the generated evaluation clearly describe merits and drawback of the each validation query in terms of possible dimensions like __Relevance__, __Factual correctness__, __Completeness__, __Clarity and coherence__ etc. w.r.t. the respective reference answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_validation_questions_anaswers(pdf_path, num_question_answers=10):\n",
    "    \"\"\"\n",
    "    Generate question-answer pairs by analyzing the content of a PDF document.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): path to the PDF file\n",
    "        num_question_answers (int): Number of question-answer pairs to generate\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: List of question-answer pairs\n",
    "    \"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"You are an expert on Retrieval-Augmented Generation (RAG) systems. Based on the provided contents from the document,\n",
    "    generate a set of question-answer pairs. The questions should be relevant to the content and cover various aspects including the main idea, reasoning, and image interpretation.\n",
    "    The questions should be strinctly based the content of the document. \n",
    "\n",
    "    Generate 10 question-answer pairs:\n",
    "    - 3 factual questions (one about hallucination in LLMs),\n",
    "    - 3 analytical questions (critical thinking),\n",
    "    - 4 image-based questions.\n",
    "    Categorize also the question-answer pairs as \"factual\", \"analytical\", or \"image-based\".\n",
    "    Answers should be brief and to the point, and image questions should refer to the figures explicitly.\"\"\"\n",
    "\n",
    "    # Extract text and captions\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "    image_captions = []\n",
    "\n",
    "    for page in doc:\n",
    "        full_text += page.get_text()\n",
    "        for block in page.get_text(\"dict\")[\"blocks\"]:\n",
    "            if block[\"type\"] == 0:\n",
    "                for line in block[\"lines\"]:\n",
    "                    line_text = \" \".join([span[\"text\"] for span in line[\"spans\"]])\n",
    "                    if line_text.lower().startswith(\"figure\"):\n",
    "                        image_captions.append(line_text)\n",
    "\n",
    "    doc.close()\n",
    "\n",
    "    user_prompt = f\"\"\"Document Text:\n",
    "{full_text[:4000]}  # Truncated to first 4000 chars to fit prompt length\n",
    "\n",
    "Image Captions:\n",
    "{chr(10).join(image_captions)}\n",
    "\n",
    "Generate {num_question_answers} question-answer pairs based on the above content.\n",
    "\"\"\"\n",
    "   \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    # Extract the content from the response\n",
    "    content = response.choices[0].message.content\n",
    "    question_type = None\n",
    "    qa_list = []\n",
    "\n",
    "    sections = re.split(r\"### (Factual|Analytical|Image-Based) Questions\", content)\n",
    "\n",
    "    # Skip the initial empty or heading text\n",
    "    for i in range(1, len(sections), 2):\n",
    "        question_type = sections[i].strip()\n",
    "        section_text = sections[i + 1]\n",
    "\n",
    "        # Find question-answer pairs\n",
    "        qa_pairs = re.findall(\n",
    "            r\"\\*\\*Question:\\*\\*\\s*(.*?)\\n\\s*-\\s*\\*\\*Answer:\\*\\*\\s*(.*?)(?=\\n\\d+\\.|\\Z)\",\n",
    "            section_text,\n",
    "            flags=re.DOTALL\n",
    "        )\n",
    "\n",
    "        for question, answer in qa_pairs:\n",
    "            qa_list.append({\n",
    "                \"question\": question.strip(),\n",
    "                \"answer\": answer.strip(),\n",
    "                \"type\": question_type\n",
    "            })\n",
    "        \n",
    "    # Save the question-answer pairs to a JSON file\n",
    "    with open(\"data/val.json\", \"w\") as f:\n",
    "        json.dump(qa_list, f, indent=4)\n",
    "    return qa_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_responses(query, vector_response, vector_response_with_images, bm25_response, fusion_response, reference_answer=None):\n",
    "    \"\"\"\n",
    "    Evaluate the responses from different retrieval methods.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_response (str): Response from vector-only RAG based on only text\n",
    "        vector_response_with_images (str): Response from vector-only RAG based on text and image captions\n",
    "        bm25_response (str): Response from BM25-only RAG\n",
    "        fusion_response (str): Response from fusion RAG\n",
    "        reference_answer (str, optional): Reference answer\n",
    "        \n",
    "    Returns:\n",
    "        str: Evaluation of responses\n",
    "    \"\"\"\n",
    "    # System prompt for the evaluator to guide the evaluation process\n",
    "    system_prompt = \"\"\"You are an expert evaluator of RAG systems. Compare responses from three different retrieval approaches:\n",
    "    1. Vector-based retrieval: Uses semantic similarity for document retrieval\n",
    "    2. Vector-based retrieval with images: Uses semantic similarity and image captions\n",
    "    2. BM25 keyword retrieval: Uses keyword matching for document retrieval\n",
    "    3. Fusion retrieval: Combines both vector and keyword approaches\n",
    "\n",
    "    Evaluate the responses based on:\n",
    "    - Relevance to the query\n",
    "    - Factual correctness\n",
    "    - Comprehensiveness\n",
    "    - Clarity and coherence\n",
    "    - \n",
    "    \"\"\"\n",
    "\n",
    "    # User prompt containing the query and responses\n",
    "    user_prompt = f\"\"\"Query: {query}\n",
    "\n",
    "    Vector-based response:\n",
    "    {vector_response}\n",
    "\n",
    "    Vector-based response with images:\n",
    "    {vector_response_with_images}\n",
    "\n",
    "    BM25 keyword response:\n",
    "    {bm25_response}\n",
    "\n",
    "    Fusion response:\n",
    "    {fusion_response}\n",
    "    \"\"\"\n",
    "\n",
    "    # Add reference answer to the prompt if provided\n",
    "    if reference_answer:\n",
    "        user_prompt += f\"\"\"\n",
    "            Reference answer:\n",
    "            {reference_answer}\n",
    "        \"\"\"\n",
    "\n",
    "    # Add instructions for detailed comparison to the user prompt\n",
    "    user_prompt += \"\"\"\n",
    "    Please provide a detailed comparison of these three responses. Which approach performed best for this query and why?\n",
    "    Be specific about the strengths and weaknesses of each approach for this particular query.\n",
    "    At the end pick a answer which retrieval approaches is the best for the given query.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate the evaluation using meta-llama/Llama-3.2-3B-Instruct\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",  # Specify the model to use\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},  # System message to guide the evaluator\n",
    "            {\"role\": \"user\", \"content\": user_prompt}  # User message with query and responses\n",
    "        ],\n",
    "        temperature=0  # Set the temperature for response generation\n",
    "    )\n",
    "    \n",
    "    # Return the generated evaluation content\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_retrieval_methods(query,vector_store_with_text_only, vector_store_with_image_captions, chunks, bm25_index, k=5, alpha=0.5, reference_answer=None):\n",
    "    \"\"\"\n",
    "    Compare different retrieval methods for a query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        chunks (List[Dict]): Text chunks\n",
    "        vector_store_with_text_only :Simple VectorStore consisting of text only\n",
    "        vector_store_with_image_captions: Multimodal VectorStore consisting of text and image data\n",
    "        bm25_index (BM25Okapi): BM25 index\n",
    "        k (int): Number of documents to retrieve\n",
    "        alpha (float): Weight for vector scores in fusion retrieval\n",
    "        reference_answer (str, optional): Reference answer for comparison\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Comparison results of different retrieval methods\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Comparing retrieval methods for query: {query} ===\\n\")\n",
    "    \n",
    "    # Run vector-only RAG - only for text\n",
    "    print(\"\\nRunning vector-only RAG with text only...\")\n",
    "    vector_result = vector_only_rag(query, vector_store_with_text_only, k)\n",
    "\n",
    "    # Run vector-only RAG - both text and image captions\n",
    "    print(\"\\nRunning vector-only RAG with text and image captions...\")\n",
    "    vector_result_with_images = vector_only_rag(query, vector_store_with_image_captions, k)\n",
    "    \n",
    "    # Run BM25-only RAG\n",
    "    print(\"\\nRunning BM25-only RAG...\")\n",
    "    bm25_result = bm25_only_rag(query, chunks, bm25_index, k)\n",
    "    \n",
    "    # Run fusion RAG\n",
    "    print(\"\\nRunning fusion RAG with text and image...\")\n",
    "    fusion_result = answer_with_multimodal_fusion_rag(query, chunks, vector_store_with_image_captions, bm25_index, k, alpha)\n",
    "    \n",
    "    # Compare responses from different retrieval methods\n",
    "    print(\"\\nComparing responses...\")\n",
    "    comparison = evaluate_responses(\n",
    "        query, \n",
    "        vector_result[\"response\"], \n",
    "        vector_result_with_images[\"response\"],\n",
    "        bm25_result[\"response\"], \n",
    "        fusion_result[\"response\"],\n",
    "        reference_answer\n",
    "    )\n",
    "    \n",
    "    # Return the comparison results\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"vector_result\": vector_result,\n",
    "        \"vector_result_with_images\": vector_result_with_images,\n",
    "        \"bm25_result\": bm25_result,\n",
    "        \"fusion_result\": fusion_result,\n",
    "        \"comparison\": comparison\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Evaluation Pipeline\n",
    "\n",
    "- For all the test query we generate a structured,comparative Evaluation results for 4 different retrieval strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fusion_retrieval(pdf_path, test_queries, reference_answers=None, k=5, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate fusion retrieval compared to other methods.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        test_queries (List[str]): List of test queries\n",
    "        reference_answers (List[str], optional): Reference answers\n",
    "        k (int): Number of documents to retrieve\n",
    "        alpha (float): Weight for vector scores in fusion retrieval\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Evaluation results\n",
    "    \"\"\"\n",
    "    print(\"=== EVALUATING FUSION RETRIEVAL ===\\n\")\n",
    "    \n",
    "    # Process the document to extract text, create chunks, and build vector and BM25 indices with out images\n",
    "    chunks, vector_store_wo_image, bm25_index, _ = process_document(pdf_path, False)\n",
    "\n",
    "    # Process the document to extract text, create chunks, and build vector and BM25 indices\n",
    "    chunks, vector_store_with_image, bm25_index, _ = process_document(pdf_path)\n",
    "    \n",
    "    # Initialize a list to store results for each query\n",
    "    results = []\n",
    "    \n",
    "    # Iterate over each test query\n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\n\\n=== Evaluating Query {i+1}/{len(test_queries)} ===\")\n",
    "        print(f\"Query: {query}\")\n",
    "        \n",
    "        # Get the reference answer if available\n",
    "        reference = None\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            reference = reference_answers[i]\n",
    "        \n",
    "        # Compare retrieval methods for the current query\n",
    "        comparison = compare_retrieval_methods(\n",
    "            query, \n",
    "            vector_store_wo_image, \n",
    "            vector_store_with_image,\n",
    "            chunks,\n",
    "            bm25_index, \n",
    "            k=k, \n",
    "            alpha=alpha,\n",
    "            reference_answer=reference\n",
    "        )\n",
    "        \n",
    "        # Append the comparison results to the results list\n",
    "        results.append(comparison)\n",
    "        \n",
    "        # Print the responses from different retrieval methods\n",
    "        print(\"\\n=== Vector-based Response with out image ===\")\n",
    "        print(comparison[\"vector_result\"][\"response\"])\n",
    "        \n",
    "        # Print the responses from different retrieval methods\n",
    "        print(\"\\n=== Vector-based Response with image ===\")\n",
    "        print(comparison[\"vector_result_with_images\"][\"response\"])\n",
    "\n",
    "        print(\"\\n=== BM25 Response ===\")\n",
    "        print(comparison[\"bm25_result\"][\"response\"])\n",
    "        \n",
    "        print(\"\\n=== Fusion Response ===\")\n",
    "        print(comparison[\"fusion_result\"][\"response\"])\n",
    "        \n",
    "        print(\"\\n=== Comparison ===\")\n",
    "        print(comparison[\"comparison\"])\n",
    "    \n",
    "    # Generate an overall analysis of the fusion retrieval performance\n",
    "    overall_analysis = generate_overall_analysis(results)\n",
    "    \n",
    "    # Return the results and overall analysis\n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"overall_analysis\": overall_analysis\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_overall_analysis(results):\n",
    "    \"\"\"\n",
    "    Generate an overall analysis of retrieval strategies.\n",
    "    \n",
    "    Args:\n",
    "        results (List[Dict]): Results from evaluating queries\n",
    "        \n",
    "    Returns:\n",
    "        str: Overall analysis\n",
    "    \"\"\"\n",
    "    # System prompt to guide the evaluation process\n",
    "    system_prompt = \"\"\"You are an expert at evaluating information retrieval systems. \n",
    "    Based on multiple test queries, provide an overall analysis comparing three retrieval approaches:\n",
    "    1. Vector-based retrieval with out metadata of result having no 'type' as 'image' and rather have all 'type' as 'text' (semantic similarity)\n",
    "    2. Vector-based retrieval with metadata of result may have 'type' as 'image' as well as 'type' as 'text' (semantic similarity)\n",
    "    3. BM25 keyword retrieval (keyword matching)\n",
    "    4. Fusion retrieval (combination of both)\n",
    "\n",
    "    Focus on:\n",
    "    1. Types of queries where each approach performs best\n",
    "    2. Overall strengths and weaknesses of each approach\n",
    "    3. How fusion retrieval balances the trade-offs\n",
    "    4. How fusion retrieval with image based answers provides advantages over the individual methods\n",
    "    5. Recommendations for when to use each approach\n",
    "    \n",
    "    At the end generate a summary of the analysis in a tabular format in overall which retrieval approaches is the best for most of questions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a summary of evaluations for each query\n",
    "    evaluations_summary = \"\"\n",
    "    for i, result in enumerate(results):\n",
    "        evaluations_summary += f\"Query {i+1}: {result['query']}\\n\"\n",
    "        evaluations_summary += f\"Comparison Summary: {result['comparison'][:200]}...\\n\\n\"\n",
    "\n",
    "    # User prompt containing the evaluations summary\n",
    "    user_prompt = f\"\"\"Based on the following evaluations of different retrieval methods across {len(results)} queries, \n",
    "    provide an overall analysis comparing these three approaches:\n",
    "\n",
    "    {evaluations_summary}\n",
    "\n",
    "    Please provide a comprehensive analysis of vector-based relying only on texts, vector-based relying both on text and images, BM25, and fusion retrieval approaches,\n",
    "    highlighting when and why fusion retrieval provides advantages over the individual methods.\"\"\"\n",
    "\n",
    "    # Generate the overall analysis using meta-llama/Llama-3.2-3B-Instruct\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Return the generated analysis content\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Fusion Retrieval\n",
    "\n",
    "- Running the evaluator for all the reference queries and respective answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EVALUATING FUSION RETRIEVAL ===\n",
      "\n",
      "Extracting content from /Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf...\n",
      "Extracted 42 text segments and 0 images\n",
      "Generating captions for 0 images...\n",
      "Created 222 text chunks\n",
      "Creating embeddings for all content...\n",
      "Added 222 items to vector store (222 text chunks, 0 image captions)\n",
      "Created BM25 index with 222 documents\n",
      "Extracting content from /Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf...\n",
      "Extracted 42 text segments and 19 images\n",
      "Generating captions for 19 images...\n",
      "Processing image 1/19...\n",
      "Processing image 2/19...\n",
      "Processing image 3/19...\n",
      "Processing image 4/19...\n",
      "Processing image 5/19...\n",
      "Processing image 6/19...\n",
      "Processing image 7/19...\n",
      "Processing image 8/19...\n",
      "Processing image 9/19...\n",
      "Processing image 10/19...\n",
      "Processing image 11/19...\n",
      "Processing image 12/19...\n",
      "Processing image 13/19...\n",
      "Processing image 14/19...\n",
      "Processing image 15/19...\n",
      "Processing image 16/19...\n",
      "Processing image 17/19...\n",
      "Processing image 18/19...\n",
      "Processing image 19/19...\n",
      "Created 222 text chunks\n",
      "Creating embeddings for all content...\n",
      "Added 241 items to vector store (222 text chunks, 19 image captions)\n",
      "Created BM25 index with 222 documents\n",
      "\n",
      "\n",
      "=== Evaluating Query 1/3 ===\n",
      "Query: Why might Modular RAG offer a significant advantage over Naive or Advanced RAG in real-world applications?\n",
      "\n",
      "=== Comparing retrieval methods for query: Why might Modular RAG offer a significant advantage over Naive or Advanced RAG in real-world applications? ===\n",
      "\n",
      "\n",
      "Running vector-only RAG with text only...\n",
      "Retrieved 5 documents using vector search\n",
      "{'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 4, 'type': 'text', 'chunk_index': 3, 'chunk_count': 5, 'index': 19}, 'similarity': 0.6932824799232403, 'text': 'ons like restructured RAG modules [13] and rearranged\\nRAG pipelines [14] have been introduced to tackle specific\\nchallenges. The shift towards a modular RAG approach is\\nbecoming prevalent, supporting both sequential processing and\\nintegrated end-to-end training across its components. Despite\\nits distinctiveness, Modular RAG builds upon the foundational\\nprinciples of Advanced and Naive RAG, illustrating a progres-\\nsion and refinement within the RAG family.\\n1) New Modules: The Modular RAG framework introduces\\nadditional specialized components to enhance retrieval and\\nprocessing capabilities. The Search module adapts to spe-\\ncific scenarios, enabling direct searches across various data\\nsources like search engines, databases, and knowledge graphs,\\nusing LLM-generated code and query languages [15]. RAG-\\nFusion addresses traditional search limitations by employing\\na multi-query strategy that expands user queries into diverse\\nperspectives, utilizing parallel vector searches and intelligent\\nre'}\n",
      "{'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 5, 'type': 'text', 'chunk_index': 1, 'chunk_count': 8, 'index': 22}, 'similarity': 0.653086649775472, 'text': 'of the information retrieved, catering to a wide array of tasks\\nand queries with enhanced precision and flexibility.\\n2) New Patterns: Modular RAG offers remarkable adapt-\\nability by allowing module substitution or reconfiguration\\nto address specific challenges. This goes beyond the fixed\\nstructures of Naive and Advanced RAG, characterized by a\\nsimple “Retrieve” and “Read” mechanism. Moreover, Modular\\nRAG expands this flexibility by integrating new modules or\\nadjusting interaction flow among existing ones, enhancing its\\napplicability across different tasks.\\nInnovations such as the Rewrite-Retrieve-Read [7]model\\nleverage the LLM’s capabilities to refine retrieval queries\\nthrough a rewriting module and a LM-feedback mechanism\\nto update rewriting model., improving task performance.\\nSimilarly, approaches like Generate-Read [13] replace tradi-\\ntional retrieval with LLM-generated content, while Recite-\\nRead [22] emphasizes retrieval from model weights, enhanc-\\ning the model’s ability to handl'}\n",
      "{'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 5, 'type': 'text', 'chunk_index': 0, 'chunk_count': 8, 'index': 21}, 'similarity': 0.623022493744531, 'text': '5\\naligns the text more closely with data distribution through iter-\\native self-enhancement [17], [18]. Routing in the RAG system\\nnavigates through diverse data sources, selecting the optimal\\npathway for a query, whether it involves summarization,\\nspecific database searches, or merging different information\\nstreams [19]. The Predict module aims to reduce redundancy\\nand noise by generating context directly through the LLM,\\nensuring relevance and accuracy [13]. Lastly, the Task Adapter\\nmodule tailors RAG to various downstream tasks, automating\\nprompt retrieval for zero-shot inputs and creating task-specific\\nretrievers through few-shot query generation [20], [21] .This\\ncomprehensive approach not only streamlines the retrieval pro-\\ncess but also significantly improves the quality and relevance\\nof the information retrieved, catering to a wide array of tasks\\nand queries with enhanced precision and flexibility.\\n2) New Patterns: Modular RAG offers remarkable adapt-\\nability by allowing module su'}\n",
      "{'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 4, 'type': 'text', 'chunk_index': 2, 'chunk_count': 5, 'index': 18}, 'similarity': 0.613988616846273, 'text': 'ey strategy. This concept has been implemented in frame-\\nworks such as LlamaIndex2, LangChain3, and HayStack [12].\\nFeeding all relevant documents directly into LLMs can lead\\nto information overload, diluting the focus on key details with\\nirrelevant content.To mitigate this, post-retrieval efforts con-\\ncentrate on selecting the essential information, emphasizing\\ncritical sections, and shortening the context to be processed.\\n2https://www.llamaindex.ai\\n3https://www.langchain.com/\\nC. Modular RAG\\nThe modular RAG architecture advances beyond the for-\\nmer two RAG paradigms, offering enhanced adaptability and\\nversatility. It incorporates diverse strategies for improving its\\ncomponents, such as adding a search module for similarity\\nsearches and refining the retriever through fine-tuning. Inno-\\nvations like restructured RAG modules [13] and rearranged\\nRAG pipelines [14] have been introduced to tackle specific\\nchallenges. The shift towards a modular RAG approach is\\nbecoming prevalent, supporting '}\n",
      "{'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 5, 'type': 'text', 'chunk_index': 3, 'chunk_count': 8, 'index': 24}, 'similarity': 0.6124295146641021, 'text': 'functionality, illustrating a\\nsophisticated understanding of enhancing module synergy.\\nThe flexible orchestration of Modular RAG Flow showcases\\nthe benefits of adaptive retrieval through techniques such as\\nFLARE [24] and Self-RAG [25]. This approach transcends\\nthe fixed RAG retrieval process by evaluating the necessity\\nof retrieval based on different scenarios. Another benefit of\\na flexible architecture is that the RAG system can more\\neasily integrate with other technologies (such as fine-tuning\\nor reinforcement learning) [26]. For example, this can involve\\nfine-tuning the retriever for better retrieval results, fine-tuning\\nthe generator for more personalized outputs, or engaging in\\ncollaborative fine-tuning [27].\\nD. RAG vs Fine-tuning\\nThe augmentation of LLMs has attracted considerable atten-\\ntion due to their growing prevalence. Among the optimization\\nmethods for LLMs, RAG is often compared with Fine-tuning\\n(FT) and prompt engineering. Each method has distinct charac-\\nteristics as il'}\n",
      "\n",
      "Running vector-only RAG with text and image captions...\n",
      "Retrieved 5 documents using vector search\n",
      "{'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 4, 'type': 'text', 'chunk_index': 3, 'chunk_count': 5, 'index': 19}, 'similarity': 0.6933491539786178, 'text': 'ons like restructured RAG modules [13] and rearranged\\nRAG pipelines [14] have been introduced to tackle specific\\nchallenges. The shift towards a modular RAG approach is\\nbecoming prevalent, supporting both sequential processing and\\nintegrated end-to-end training across its components. Despite\\nits distinctiveness, Modular RAG builds upon the foundational\\nprinciples of Advanced and Naive RAG, illustrating a progres-\\nsion and refinement within the RAG family.\\n1) New Modules: The Modular RAG framework introduces\\nadditional specialized components to enhance retrieval and\\nprocessing capabilities. The Search module adapts to spe-\\ncific scenarios, enabling direct searches across various data\\nsources like search engines, databases, and knowledge graphs,\\nusing LLM-generated code and query languages [15]. RAG-\\nFusion addresses traditional search limitations by employing\\na multi-query strategy that expands user queries into diverse\\nperspectives, utilizing parallel vector searches and intelligent\\nre'}\n",
      "{'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 5, 'type': 'text', 'chunk_index': 1, 'chunk_count': 8, 'index': 22}, 'similarity': 0.6529748707000053, 'text': 'of the information retrieved, catering to a wide array of tasks\\nand queries with enhanced precision and flexibility.\\n2) New Patterns: Modular RAG offers remarkable adapt-\\nability by allowing module substitution or reconfiguration\\nto address specific challenges. This goes beyond the fixed\\nstructures of Naive and Advanced RAG, characterized by a\\nsimple “Retrieve” and “Read” mechanism. Moreover, Modular\\nRAG expands this flexibility by integrating new modules or\\nadjusting interaction flow among existing ones, enhancing its\\napplicability across different tasks.\\nInnovations such as the Rewrite-Retrieve-Read [7]model\\nleverage the LLM’s capabilities to refine retrieval queries\\nthrough a rewriting module and a LM-feedback mechanism\\nto update rewriting model., improving task performance.\\nSimilarly, approaches like Generate-Read [13] replace tradi-\\ntional retrieval with LLM-generated content, while Recite-\\nRead [22] emphasizes retrieval from model weights, enhanc-\\ning the model’s ability to handl'}\n",
      "{'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 5, 'type': 'text', 'chunk_index': 0, 'chunk_count': 8, 'index': 21}, 'similarity': 0.6230364851043182, 'text': '5\\naligns the text more closely with data distribution through iter-\\native self-enhancement [17], [18]. Routing in the RAG system\\nnavigates through diverse data sources, selecting the optimal\\npathway for a query, whether it involves summarization,\\nspecific database searches, or merging different information\\nstreams [19]. The Predict module aims to reduce redundancy\\nand noise by generating context directly through the LLM,\\nensuring relevance and accuracy [13]. Lastly, the Task Adapter\\nmodule tailors RAG to various downstream tasks, automating\\nprompt retrieval for zero-shot inputs and creating task-specific\\nretrievers through few-shot query generation [20], [21] .This\\ncomprehensive approach not only streamlines the retrieval pro-\\ncess but also significantly improves the quality and relevance\\nof the information retrieved, catering to a wide array of tasks\\nand queries with enhanced precision and flexibility.\\n2) New Patterns: Modular RAG offers remarkable adapt-\\nability by allowing module su'}\n",
      "{'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 4, 'type': 'text', 'chunk_index': 2, 'chunk_count': 5, 'index': 18}, 'similarity': 0.614007228138263, 'text': 'ey strategy. This concept has been implemented in frame-\\nworks such as LlamaIndex2, LangChain3, and HayStack [12].\\nFeeding all relevant documents directly into LLMs can lead\\nto information overload, diluting the focus on key details with\\nirrelevant content.To mitigate this, post-retrieval efforts con-\\ncentrate on selecting the essential information, emphasizing\\ncritical sections, and shortening the context to be processed.\\n2https://www.llamaindex.ai\\n3https://www.langchain.com/\\nC. Modular RAG\\nThe modular RAG architecture advances beyond the for-\\nmer two RAG paradigms, offering enhanced adaptability and\\nversatility. It incorporates diverse strategies for improving its\\ncomponents, such as adding a search module for similarity\\nsearches and refining the retriever through fine-tuning. Inno-\\nvations like restructured RAG modules [13] and rearranged\\nRAG pipelines [14] have been introduced to tackle specific\\nchallenges. The shift towards a modular RAG approach is\\nbecoming prevalent, supporting '}\n",
      "{'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 5, 'type': 'text', 'chunk_index': 3, 'chunk_count': 8, 'index': 24}, 'similarity': 0.6124196714558215, 'text': 'functionality, illustrating a\\nsophisticated understanding of enhancing module synergy.\\nThe flexible orchestration of Modular RAG Flow showcases\\nthe benefits of adaptive retrieval through techniques such as\\nFLARE [24] and Self-RAG [25]. This approach transcends\\nthe fixed RAG retrieval process by evaluating the necessity\\nof retrieval based on different scenarios. Another benefit of\\na flexible architecture is that the RAG system can more\\neasily integrate with other technologies (such as fine-tuning\\nor reinforcement learning) [26]. For example, this can involve\\nfine-tuning the retriever for better retrieval results, fine-tuning\\nthe generator for more personalized outputs, or engaging in\\ncollaborative fine-tuning [27].\\nD. RAG vs Fine-tuning\\nThe augmentation of LLMs has attracted considerable atten-\\ntion due to their growing prevalence. Among the optimization\\nmethods for LLMs, RAG is often compared with Fine-tuning\\n(FT) and prompt engineering. Each method has distinct charac-\\nteristics as il'}\n",
      "\n",
      "Running BM25-only RAG...\n",
      "{'text': 'les related\\nto the user’s query. These articles, combined with the original\\nquestion, form a comprehensive prompt that empowers LLMs\\nto generate a well-informed answer.\\nThe RAG research paradigm is continuously evolving, and\\nwe categorize it into three stages: Naive RAG, Advanced\\nRAG, and Modular RAG, as showed in Figure 3. Despite\\nRAG method are cost-effective and surpass the performance\\nof the native LLM, they also exhibit several limitations.\\nThe development of Advanced RAG and Modular RAG is\\na response to these specific shortcomings in Naive RAG.\\nA. Naive RAG\\nThe Naive RAG research paradigm represents the earli-\\nest methodology, which gained prominence shortly after the', 'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 2, 'type': 'text', 'chunk_index': 3, 'chunk_count': 4, 'index': 10}, 'bm25_score': 21.81768712968065}\n",
      "{'text': 'may not suffice to acquire adequate context information.\\nMoreover, there’s a concern that generation models might\\noverly rely on augmented information, leading to outputs that\\nsimply echo retrieved content without adding insightful or\\nsynthesized information.\\nB. Advanced RAG\\nAdvanced RAG introduces specific improvements to over-\\ncome the limitations of Naive RAG. Focusing on enhancing re-\\ntrieval quality, it employs pre-retrieval and post-retrieval strate-\\ngies. To tackle the indexing issues, Advanced RAG refines\\nits indexing techniques through the use of a sliding window\\napproach, fine-grained segmentation, and the incorporation of\\nmetadata. Additionally, it incorporates several optimization\\nmethods to streamline the retrieval process [8].', 'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 3, 'type': 'text', 'chunk_index': 4, 'chunk_count': 5, 'index': 15}, 'bm25_score': 19.1446790060016}\n",
      "{'text': 'of the information retrieved, catering to a wide array of tasks\\nand queries with enhanced precision and flexibility.\\n2) New Patterns: Modular RAG offers remarkable adapt-\\nability by allowing module substitution or reconfiguration\\nto address specific challenges. This goes beyond the fixed\\nstructures of Naive and Advanced RAG, characterized by a\\nsimple “Retrieve” and “Read” mechanism. Moreover, Modular\\nRAG expands this flexibility by integrating new modules or\\nadjusting interaction flow among existing ones, enhancing its\\napplicability across different tasks.\\nInnovations such as the Rewrite-Retrieve-Read [7]model\\nleverage the LLM’s capabilities to refine retrieval queries\\nthrough a rewriting module and a LM-feedback mechanism\\nto update rewriting model., improving task performance.\\nSimilarly, approaches like Generate-Read [13] replace tradi-\\ntional retrieval with LLM-generated content, while Recite-\\nRead [22] emphasizes retrieval from model weights, enhanc-\\ning the model’s ability to handl', 'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 5, 'type': 'text', 'chunk_index': 1, 'chunk_count': 8, 'index': 22}, 'bm25_score': 15.887466296606272}\n",
      "{'text': '4\\nFig. 3.\\nComparison between the three paradigms of RAG. (Left) Naive RAG mainly consists of three parts: indexing, retrieval and generation. (Middle)\\nAdvanced RAG proposes multiple optimization strategies around pre-retrieval and post-retrieval, with a process similar to the Naive RAG, still following a\\nchain-like structure. (Right) Modular RAG inherits and develops from the previous paradigm, showcasing greater flexibility overall. This is evident in the\\nintroduction of multiple specific functional modules and the replacement of existing modules. The overall process is not limited to sequential retrieval and\\ngeneration; it includes methods such as iterative and adaptive retrieval.\\nPre-retrieval process. In this stage, the primary focus is\\non optimizing the indexing structure and the original query.\\nThe goal of optimizing indexing is to enhance the quality of\\nthe content being indexed. This involves strategies: enhancing\\ndata granularity, optimizing index structures, adding metadata,\\n', 'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 4, 'type': 'text', 'chunk_index': 0, 'chunk_count': 5, 'index': 16}, 'bm25_score': 15.160870444883315}\n",
      "{'text': 'ons like restructured RAG modules [13] and rearranged\\nRAG pipelines [14] have been introduced to tackle specific\\nchallenges. The shift towards a modular RAG approach is\\nbecoming prevalent, supporting both sequential processing and\\nintegrated end-to-end training across its components. Despite\\nits distinctiveness, Modular RAG builds upon the foundational\\nprinciples of Advanced and Naive RAG, illustrating a progres-\\nsion and refinement within the RAG family.\\n1) New Modules: The Modular RAG framework introduces\\nadditional specialized components to enhance retrieval and\\nprocessing capabilities. The Search module adapts to spe-\\ncific scenarios, enabling direct searches across various data\\nsources like search engines, databases, and knowledge graphs,\\nusing LLM-generated code and query languages [15]. RAG-\\nFusion addresses traditional search limitations by employing\\na multi-query strategy that expands user queries into diverse\\nperspectives, utilizing parallel vector searches and intelligent\\nre', 'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 4, 'type': 'text', 'chunk_index': 3, 'chunk_count': 5, 'index': 19}, 'bm25_score': 14.892750495912981}\n",
      "\n",
      "Running fusion RAG with text and image...\n",
      "Performing fusion retrieval for query: Why might Modular RAG offer a significant advantage over Naive or Advanced RAG in real-world applications?\n",
      "Retrieved 222 relevant items (215 text, 7 image captions)\n",
      "Retrieved 5 documents with fusion retrieval\n",
      "{'text': 'les related\\nto the user’s query. These articles, combined with the original\\nquestion, form a comprehensive prompt that empowers LLMs\\nto generate a well-informed answer.\\nThe RAG research paradigm is continuously evolving, and\\nwe categorize it into three stages: Naive RAG, Advanced\\nRAG, and Modular RAG, as showed in Figure 3. Despite\\nRAG method are cost-effective and surpass the performance\\nof the native LLM, they also exhibit several limitations.\\nThe development of Advanced RAG and Modular RAG is\\na response to these specific shortcomings in Naive RAG.\\nA. Naive RAG\\nThe Naive RAG research paradigm represents the earli-\\nest methodology, which gained prominence shortly after the', 'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 2, 'type': 'text', 'chunk_index': 3, 'chunk_count': 4, 'index': 10}, 'vector_score': 0.5900237004769713, 'bm25_score': 21.81768712968065, 'index': 10, 'combined_score': 0.9254639152022955}\n",
      "{'text': 'may not suffice to acquire adequate context information.\\nMoreover, there’s a concern that generation models might\\noverly rely on augmented information, leading to outputs that\\nsimply echo retrieved content without adding insightful or\\nsynthesized information.\\nB. Advanced RAG\\nAdvanced RAG introduces specific improvements to over-\\ncome the limitations of Naive RAG. Focusing on enhancing re-\\ntrieval quality, it employs pre-retrieval and post-retrieval strate-\\ngies. To tackle the indexing issues, Advanced RAG refines\\nits indexing techniques through the use of a sliding window\\napproach, fine-grained segmentation, and the incorporation of\\nmetadata. Additionally, it incorporates several optimization\\nmethods to streamline the retrieval process [8].', 'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 3, 'type': 'text', 'chunk_index': 4, 'chunk_count': 5, 'index': 15}, 'vector_score': 0.587807423282185, 'bm25_score': 19.1446790060016, 'index': 15, 'combined_score': 0.8626079408350211}\n",
      "{'text': 'ons like restructured RAG modules [13] and rearranged\\nRAG pipelines [14] have been introduced to tackle specific\\nchallenges. The shift towards a modular RAG approach is\\nbecoming prevalent, supporting both sequential processing and\\nintegrated end-to-end training across its components. Despite\\nits distinctiveness, Modular RAG builds upon the foundational\\nprinciples of Advanced and Naive RAG, illustrating a progres-\\nsion and refinement within the RAG family.\\n1) New Modules: The Modular RAG framework introduces\\nadditional specialized components to enhance retrieval and\\nprocessing capabilities. The Search module adapts to spe-\\ncific scenarios, enabling direct searches across various data\\nsources like search engines, databases, and knowledge graphs,\\nusing LLM-generated code and query languages [15]. RAG-\\nFusion addresses traditional search limitations by employing\\na multi-query strategy that expands user queries into diverse\\nperspectives, utilizing parallel vector searches and intelligent\\nre', 'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 4, 'type': 'text', 'chunk_index': 3, 'chunk_count': 5, 'index': 19}, 'vector_score': 0.693388640690414, 'bm25_score': 14.892750495912981, 'index': 19, 'combined_score': 0.8412999298668533}\n",
      "{'text': 'of the information retrieved, catering to a wide array of tasks\\nand queries with enhanced precision and flexibility.\\n2) New Patterns: Modular RAG offers remarkable adapt-\\nability by allowing module substitution or reconfiguration\\nto address specific challenges. This goes beyond the fixed\\nstructures of Naive and Advanced RAG, characterized by a\\nsimple “Retrieve” and “Read” mechanism. Moreover, Modular\\nRAG expands this flexibility by integrating new modules or\\nadjusting interaction flow among existing ones, enhancing its\\napplicability across different tasks.\\nInnovations such as the Rewrite-Retrieve-Read [7]model\\nleverage the LLM’s capabilities to refine retrieval queries\\nthrough a rewriting module and a LM-feedback mechanism\\nto update rewriting model., improving task performance.\\nSimilarly, approaches like Generate-Read [13] replace tradi-\\ntional retrieval with LLM-generated content, while Recite-\\nRead [22] emphasizes retrieval from model weights, enhanc-\\ning the model’s ability to handl', 'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 5, 'type': 'text', 'chunk_index': 1, 'chunk_count': 8, 'index': 22}, 'vector_score': 0.6530308488382408, 'bm25_score': 15.887466296606272, 'index': 22, 'combined_score': 0.8349941622145582}\n",
      "{'text': '4\\nFig. 3.\\nComparison between the three paradigms of RAG. (Left) Naive RAG mainly consists of three parts: indexing, retrieval and generation. (Middle)\\nAdvanced RAG proposes multiple optimization strategies around pre-retrieval and post-retrieval, with a process similar to the Naive RAG, still following a\\nchain-like structure. (Right) Modular RAG inherits and develops from the previous paradigm, showcasing greater flexibility overall. This is evident in the\\nintroduction of multiple specific functional modules and the replacement of existing modules. The overall process is not limited to sequential retrieval and\\ngeneration; it includes methods such as iterative and adaptive retrieval.\\nPre-retrieval process. In this stage, the primary focus is\\non optimizing the indexing structure and the original query.\\nThe goal of optimizing indexing is to enhance the quality of\\nthe content being indexed. This involves strategies: enhancing\\ndata granularity, optimizing index structures, adding metadata,\\n', 'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 4, 'type': 'text', 'chunk_index': 0, 'chunk_count': 5, 'index': 16}, 'vector_score': 0.595805631591824, 'bm25_score': 15.160870444883315, 'index': 16, 'combined_score': 0.7770777365337679}\n",
      "\n",
      "Comparing responses...\n",
      "\n",
      "=== Vector-based Response with out image ===\n",
      "Modular RAG offers significant advantages over Naive or Advanced RAG in real-world applications due to its enhanced adaptability, versatility, and precision. Here are the key reasons:\n",
      "\n",
      "1. **Specialized Components**: Modular RAG introduces additional specialized modules, such as the Search module, which can adapt to specific scenarios by enabling direct searches across various data sources like search engines, databases, and knowledge graphs. This is achieved using LLM-generated code and query languages, which enhances retrieval and processing capabilities.\n",
      "\n",
      "2. **Flexibility and Adaptability**: Unlike the fixed structures of Naive and Advanced RAG, Modular RAG allows for module substitution or reconfiguration to address specific challenges. This flexibility enables the integration of new modules or the adjustment of interaction flows among existing ones, making it applicable across a wide range of tasks.\n",
      "\n",
      "3. **Enhanced Retrieval Strategies**: Innovations such as the Rewrite-Retrieve-Read model and Generate-Read approach leverage LLM capabilities to refine retrieval queries and generate content, respectively. This improves task performance by enhancing the quality and relevance of the retrieved information.\n",
      "\n",
      "4. **Task-Specific Adaptation**: The Task Adapter module tailors RAG to various downstream tasks, automating prompt retrieval for zero-shot inputs and creating task-specific retrievers through few-shot query generation. This streamlines the retrieval process and improves the quality of information for diverse tasks.\n",
      "\n",
      "5. **Integration with Other Technologies**: The flexible architecture of Modular RAG allows for easier integration with other technologies, such as fine-tuning or reinforcement learning. This can lead to better retrieval results and more personalized outputs.\n",
      "\n",
      "Overall, the modular approach allows for a more sophisticated and adaptable system that can handle a wide array of tasks with enhanced precision and flexibility, making it highly advantageous in real-world applications.\n",
      "\n",
      "=== Vector-based Response with image ===\n",
      "Modular RAG offers significant advantages over Naive or Advanced RAG in real-world applications due to its enhanced adaptability, versatility, and precision. Here are some key reasons:\n",
      "\n",
      "1. **Specialized Components**: Modular RAG introduces new modules like the Search module, which can adapt to specific scenarios by enabling direct searches across various data sources using LLM-generated code and query languages. This allows for more targeted and efficient retrieval processes.\n",
      "\n",
      "2. **Flexibility and Adaptability**: Unlike the fixed structures of Naive and Advanced RAG, Modular RAG allows for module substitution or reconfiguration to address specific challenges. This flexibility enables it to integrate new modules or adjust interaction flows among existing ones, making it applicable to a broader range of tasks.\n",
      "\n",
      "3. **Enhanced Retrieval and Processing**: Innovations such as the Rewrite-Retrieve-Read model and the integration of LLM-generated content improve task performance by refining retrieval queries and enhancing the model's ability to handle diverse data sources.\n",
      "\n",
      "4. **Improved Information Quality**: The Predict module in Modular RAG reduces redundancy and noise by generating context directly through the LLM, ensuring relevance and accuracy. This leads to a more streamlined retrieval process and improves the quality and relevance of the information retrieved.\n",
      "\n",
      "5. **Integration with Other Technologies**: The flexible architecture of Modular RAG allows for easier integration with other technologies, such as fine-tuning or reinforcement learning, which can further enhance retrieval results and personalize outputs.\n",
      "\n",
      "Overall, the modular approach provides a sophisticated framework that can adapt to various real-world scenarios, offering significant improvements in precision, flexibility, and integration capabilities compared to Naive or Advanced RAG.\n",
      "\n",
      "=== BM25 Response ===\n",
      "Modular RAG offers significant advantages over Naive and Advanced RAG in real-world applications due to its enhanced flexibility and adaptability. Unlike the fixed structures of Naive and Advanced RAG, which primarily follow a \"Retrieve\" and \"Read\" mechanism, Modular RAG allows for module substitution or reconfiguration to address specific challenges. This adaptability is crucial in real-world applications where diverse and dynamic requirements are common.\n",
      "\n",
      "Key advantages of Modular RAG include:\n",
      "\n",
      "1. **New Modules and Patterns**: Modular RAG introduces specialized components, such as the Search module, which can adapt to specific scenarios by enabling direct searches across various data sources. This is achieved using LLM-generated code and query languages, which enhances retrieval and processing capabilities.\n",
      "\n",
      "2. **Iterative and Adaptive Retrieval**: The process in Modular RAG is not limited to sequential retrieval and generation. It includes iterative and adaptive retrieval methods, allowing for more nuanced and contextually relevant information processing.\n",
      "\n",
      "3. **Enhanced Precision and Flexibility**: By integrating new modules or adjusting the interaction flow among existing ones, Modular RAG can cater to a wide array of tasks and queries with improved precision and flexibility.\n",
      "\n",
      "4. **Innovative Approaches**: Techniques like the Rewrite-Retrieve-Read model leverage LLM capabilities to refine retrieval queries, improving task performance. Other approaches, such as Generate-Read and Recite-Read, replace traditional retrieval with LLM-generated content or emphasize retrieval from model weights, respectively.\n",
      "\n",
      "These features make Modular RAG particularly suited for complex, real-world applications where the ability to adapt and optimize retrieval and processing strategies is essential.\n",
      "\n",
      "=== Fusion Response ===\n",
      "Modular RAG offers significant advantages over Naive and Advanced RAG in real-world applications due to its enhanced flexibility and adaptability. Here are the key reasons:\n",
      "\n",
      "1. **Specialized Components**: Modular RAG introduces new modules that enhance retrieval and processing capabilities. For example, the Search module can adapt to specific scenarios by enabling direct searches across various data sources, such as search engines, databases, and knowledge graphs. This is achieved using LLM-generated code and query languages, which allows for more precise and contextually relevant information retrieval.\n",
      "\n",
      "2. **Multi-Query Strategy**: The RAG-Fusion component addresses traditional search limitations by employing a multi-query strategy. This expands user queries into diverse perspectives, utilizing parallel vector searches and intelligent retrieval methods, which can lead to more comprehensive and nuanced results.\n",
      "\n",
      "3. **Adaptability and Reconfiguration**: Modular RAG allows for module substitution or reconfiguration to address specific challenges. This adaptability goes beyond the fixed structures of Naive and Advanced RAG, which are characterized by a simple \"Retrieve\" and \"Read\" mechanism. The ability to integrate new modules or adjust the interaction flow among existing ones enhances its applicability across different tasks and scenarios.\n",
      "\n",
      "4. **Innovative Patterns**: Modular RAG supports innovative patterns like the Rewrite-Retrieve-Read model, which refines retrieval queries through a rewriting module and a feedback mechanism. This improves task performance by leveraging the LLM's capabilities. Other approaches, such as Generate-Read and Recite-Read, further enhance the model's ability to handle diverse tasks by replacing traditional retrieval with LLM-generated content or emphasizing retrieval from model weights.\n",
      "\n",
      "Overall, the flexibility, adaptability, and enhanced retrieval strategies of Modular RAG make it more suitable for complex and varied real-world applications compared to the more rigid structures of Naive and Advanced RAG.\n",
      "\n",
      "=== Comparison ===\n",
      "### Evaluation of Responses\n",
      "\n",
      "#### 1. Vector-based Retrieval\n",
      "\n",
      "- **Relevance to the Query**: \n",
      "  - The response is highly relevant, addressing the advantages of Modular RAG over Naive and Advanced RAG with a focus on adaptability, versatility, and precision.\n",
      "  \n",
      "- **Factual Correctness**: \n",
      "  - The response is factually correct, providing detailed explanations of Modular RAG's components and strategies.\n",
      "  \n",
      "- **Comprehensiveness**: \n",
      "  - The response is comprehensive, covering various aspects such as specialized components, flexibility, enhanced retrieval strategies, task-specific adaptation, and integration with other technologies.\n",
      "  \n",
      "- **Clarity and Coherence**: \n",
      "  - The response is clear and coherent, with well-structured points and logical flow.\n",
      "\n",
      "#### 2. Vector-based Retrieval with Images\n",
      "\n",
      "- **Relevance to the Query**: \n",
      "  - Similar to the vector-based response, this response is highly relevant, focusing on the advantages of Modular RAG.\n",
      "  \n",
      "- **Factual Correctness**: \n",
      "  - The response is factually correct, with accurate descriptions of Modular RAG's features.\n",
      "  \n",
      "- **Comprehensiveness**: \n",
      "  - The response is comprehensive, though it slightly overlaps with the vector-based response. It introduces the Predict module, which is not mentioned in the vector-based response.\n",
      "  \n",
      "- **Clarity and Coherence**: \n",
      "  - The response is clear and coherent, maintaining a logical structure throughout.\n",
      "\n",
      "#### 3. BM25 Keyword Retrieval\n",
      "\n",
      "- **Relevance to the Query**: \n",
      "  - The response is relevant, focusing on the flexibility and adaptability of Modular RAG.\n",
      "  \n",
      "- **Factual Correctness**: \n",
      "  - The response is factually correct, but it lacks some depth compared to the vector-based responses.\n",
      "  \n",
      "- **Comprehensiveness**: \n",
      "  - The response is less comprehensive, missing some details about specific modules and strategies that are present in the vector-based responses.\n",
      "  \n",
      "- **Clarity and Coherence**: \n",
      "  - The response is clear and coherent, but it could benefit from more detailed explanations.\n",
      "\n",
      "#### 4. Fusion Retrieval\n",
      "\n",
      "- **Relevance to the Query**: \n",
      "  - The response is highly relevant, addressing the key advantages of Modular RAG.\n",
      "  \n",
      "- **Factual Correctness**: \n",
      "  - The response is factually correct, accurately describing the features and strategies of Modular RAG.\n",
      "  \n",
      "- **Comprehensiveness**: \n",
      "  - The response is comprehensive, covering specialized components, multi-query strategy, adaptability, and innovative patterns.\n",
      "  \n",
      "- **Clarity and Coherence**: \n",
      "  - The response is clear and coherent, with a well-organized structure.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "- **Best Performing Approach**: **Fusion Retrieval**\n",
      "  - **Reasons**: The Fusion Retrieval response combines the strengths of both vector and keyword approaches, providing a comprehensive, clear, and coherent explanation of Modular RAG's advantages. It introduces unique elements like the multi-query strategy and innovative patterns, which enhance the response's depth and relevance.\n",
      "\n",
      "- **Strengths and Weaknesses**:\n",
      "  - **Vector-based Retrieval**: Strong in relevance, factual correctness, and comprehensiveness, but lacks the unique elements found in the Fusion response.\n",
      "  - **Vector-based Retrieval with Images**: Similar strengths to the vector-based response, with the addition of the Predict module, but still not as comprehensive as the Fusion response.\n",
      "  - **BM25 Keyword Retrieval**: Clear and relevant but less comprehensive and detailed compared to the other approaches.\n",
      "  - **Fusion Retrieval**: Best overall due to its comprehensive coverage, clarity, and inclusion of unique strategies like the multi-query approach.\n",
      "\n",
      "\n",
      "=== Evaluating Query 2/3 ===\n",
      "Query: Based on Figure 1 (RAG Technology Tree), how has RAG research evolved over time?\n",
      "\n",
      "=== Comparing retrieval methods for query: Based on Figure 1 (RAG Technology Tree), how has RAG research evolved over time? ===\n",
      "\n",
      "\n",
      "Running vector-only RAG with text only...\n",
      "Retrieved 5 documents using vector search\n",
      "{'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 16, 'type': 'text', 'chunk_index': 0, 'chunk_count': 5, 'index': 92}, 'similarity': 0.6576119894777551, 'text': '16\\nFig. 6. Summary of RAG ecosystem\\ninitial learning curve. 3) Specialization - optimizing RAG to\\nbetter serve production environments.\\nThe mutual growth of RAG models and their technology\\nstacks is evident; technological advancements continuously\\nestablish new standards for existing infrastructure. In turn,\\nenhancements to the technology stack drive the development\\nof RAG capabilities. RAG toolkits are converging into a\\nfoundational technology stack, laying the groundwork for\\nadvanced enterprise applications. However, a fully integrated,\\ncomprehensive platform concept is still in the future, requiring\\nfurther innovation and development.\\nF. Multi-modal RAG\\nRAG\\nhas\\ntranscended\\nits\\ninitial\\ntext-based\\nquestion-\\nanswering confines, embracing a diverse array of modal data.\\nThis expansion has spawned innovative multimodal models\\nthat integrate RAG concepts across various domains:\\nImage. RA-CM3 [176] stands as a pioneering multimodal\\nmodel of both retrieving and generating text and images.\\nBL'}\n",
      "{'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 2, 'type': 'text', 'chunk_index': 3, 'chunk_count': 4, 'index': 10}, 'similarity': 0.6129636585796174, 'text': 'les related\\nto the user’s query. These articles, combined with the original\\nquestion, form a comprehensive prompt that empowers LLMs\\nto generate a well-informed answer.\\nThe RAG research paradigm is continuously evolving, and\\nwe categorize it into three stages: Naive RAG, Advanced\\nRAG, and Modular RAG, as showed in Figure 3. Despite\\nRAG method are cost-effective and surpass the performance\\nof the native LLM, they also exhibit several limitations.\\nThe development of Advanced RAG and Modular RAG is\\na response to these specific shortcomings in Naive RAG.\\nA. Naive RAG\\nThe Naive RAG research paradigm represents the earli-\\nest methodology, which gained prominence shortly after the'}\n",
      "{'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 2, 'type': 'text', 'chunk_index': 0, 'chunk_count': 4, 'index': 7}, 'similarity': 0.5739685577902688, 'text': '2\\nFig. 1. Technology tree of RAG research. The stages of involving RAG mainly include pre-training, fine-tuning, and inference. With the emergence of LLMs,\\nresearch on RAG initially focused on leveraging the powerful in context learning abilities of LLMs, primarily concentrating on the inference stage. Subsequent\\nresearch has delved deeper, gradually integrating more with the fine-tuning of LLMs. Researchers have also been exploring ways to enhance language models\\nin the pre-training stage through retrieval-augmented techniques.\\nadvanced RAG, and modular RAG. This review contex-\\ntualizes the broader scope of RAG research within the\\nlandscape of LLMs.\\n• We identify and discuss the central technologies integral\\nto the RAG process, specifically focusing on the aspects\\nof “Retrieval”, “Generation” and “Augmentation”, and\\ndelve into their synergies, elucidating how these com-\\nponents intricately collaborate to form a cohesive and\\neffective RAG framework.\\n• We have summarized the current ass'}\n",
      "{'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 16, 'type': 'text', 'chunk_index': 3, 'chunk_count': 5, 'index': 95}, 'similarity': 0.5634878212026766, 'text': ' the input, enhancing performance in knowledge graph\\nquestion-answering tasks.\\nVIII. CONCLUSION\\nThe summary of this paper, as depicted in Figure 6, empha-\\nsizes RAG’s significant advancement in enhancing the capa-\\nbilities of LLMs by integrating parameterized knowledge from\\nlanguage models with extensive non-parameterized data from\\nexternal knowledge bases. The survey showcases the evolution\\nof RAG technologies and their application on many different\\ntasks. The analysis outlines three developmental paradigms\\nwithin the RAG framework: Naive, Advanced, and Modu-\\nlar RAG, each representing a progressive enhancement over\\nits predecessors. RAG’s technical integration with other AI\\nmethodologies, such as fine-tuning and reinforcement learning,\\nhas further expanded its capabilities. Despite the progress in\\nRAG technology, there are research opportunities to improve\\nits robustness and its ability to handle extended contexts.\\nRAG’s application scope is expanding into multimodal do-\\nmains, adapt'}\n",
      "{'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 16, 'type': 'text', 'chunk_index': 4, 'chunk_count': 5, 'index': 96}, 'similarity': 0.5627371324837618, 'text': 'rogress in\\nRAG technology, there are research opportunities to improve\\nits robustness and its ability to handle extended contexts.\\nRAG’s application scope is expanding into multimodal do-\\nmains, adapting its principles to interpret and process diverse\\ndata forms like images, videos, and code. This expansion high-\\nlights RAG’s significant practical implications for AI deploy-\\nment, attracting interest from academic and industrial sectors.'}\n",
      "\n",
      "Running vector-only RAG with text and image captions...\n",
      "Retrieved 5 documents using vector search\n",
      "{'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 16, 'type': 'text', 'chunk_index': 0, 'chunk_count': 5, 'index': 92}, 'similarity': 0.6587173545388791, 'text': '16\\nFig. 6. Summary of RAG ecosystem\\ninitial learning curve. 3) Specialization - optimizing RAG to\\nbetter serve production environments.\\nThe mutual growth of RAG models and their technology\\nstacks is evident; technological advancements continuously\\nestablish new standards for existing infrastructure. In turn,\\nenhancements to the technology stack drive the development\\nof RAG capabilities. RAG toolkits are converging into a\\nfoundational technology stack, laying the groundwork for\\nadvanced enterprise applications. However, a fully integrated,\\ncomprehensive platform concept is still in the future, requiring\\nfurther innovation and development.\\nF. Multi-modal RAG\\nRAG\\nhas\\ntranscended\\nits\\ninitial\\ntext-based\\nquestion-\\nanswering confines, embracing a diverse array of modal data.\\nThis expansion has spawned innovative multimodal models\\nthat integrate RAG concepts across various domains:\\nImage. RA-CM3 [176] stands as a pioneering multimodal\\nmodel of both retrieving and generating text and images.\\nBL'}\n",
      "{'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 2, 'type': 'text', 'chunk_index': 3, 'chunk_count': 4, 'index': 10}, 'similarity': 0.6121983176614669, 'text': 'les related\\nto the user’s query. These articles, combined with the original\\nquestion, form a comprehensive prompt that empowers LLMs\\nto generate a well-informed answer.\\nThe RAG research paradigm is continuously evolving, and\\nwe categorize it into three stages: Naive RAG, Advanced\\nRAG, and Modular RAG, as showed in Figure 3. Despite\\nRAG method are cost-effective and surpass the performance\\nof the native LLM, they also exhibit several limitations.\\nThe development of Advanced RAG and Modular RAG is\\na response to these specific shortcomings in Naive RAG.\\nA. Naive RAG\\nThe Naive RAG research paradigm represents the earli-\\nest methodology, which gained prominence shortly after the'}\n",
      "{'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 2, 'type': 'text', 'chunk_index': 0, 'chunk_count': 4, 'index': 7}, 'similarity': 0.5755301027249173, 'text': '2\\nFig. 1. Technology tree of RAG research. The stages of involving RAG mainly include pre-training, fine-tuning, and inference. With the emergence of LLMs,\\nresearch on RAG initially focused on leveraging the powerful in context learning abilities of LLMs, primarily concentrating on the inference stage. Subsequent\\nresearch has delved deeper, gradually integrating more with the fine-tuning of LLMs. Researchers have also been exploring ways to enhance language models\\nin the pre-training stage through retrieval-augmented techniques.\\nadvanced RAG, and modular RAG. This review contex-\\ntualizes the broader scope of RAG research within the\\nlandscape of LLMs.\\n• We identify and discuss the central technologies integral\\nto the RAG process, specifically focusing on the aspects\\nof “Retrieval”, “Generation” and “Augmentation”, and\\ndelve into their synergies, elucidating how these com-\\nponents intricately collaborate to form a cohesive and\\neffective RAG framework.\\n• We have summarized the current ass'}\n",
      "{'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 16, 'type': 'text', 'chunk_index': 3, 'chunk_count': 5, 'index': 95}, 'similarity': 0.5646111306655559, 'text': ' the input, enhancing performance in knowledge graph\\nquestion-answering tasks.\\nVIII. CONCLUSION\\nThe summary of this paper, as depicted in Figure 6, empha-\\nsizes RAG’s significant advancement in enhancing the capa-\\nbilities of LLMs by integrating parameterized knowledge from\\nlanguage models with extensive non-parameterized data from\\nexternal knowledge bases. The survey showcases the evolution\\nof RAG technologies and their application on many different\\ntasks. The analysis outlines three developmental paradigms\\nwithin the RAG framework: Naive, Advanced, and Modu-\\nlar RAG, each representing a progressive enhancement over\\nits predecessors. RAG’s technical integration with other AI\\nmethodologies, such as fine-tuning and reinforcement learning,\\nhas further expanded its capabilities. Despite the progress in\\nRAG technology, there are research opportunities to improve\\nits robustness and its ability to handle extended contexts.\\nRAG’s application scope is expanding into multimodal do-\\nmains, adapt'}\n",
      "{'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 16, 'type': 'text', 'chunk_index': 4, 'chunk_count': 5, 'index': 96}, 'similarity': 0.5641277341216719, 'text': 'rogress in\\nRAG technology, there are research opportunities to improve\\nits robustness and its ability to handle extended contexts.\\nRAG’s application scope is expanding into multimodal do-\\nmains, adapting its principles to interpret and process diverse\\ndata forms like images, videos, and code. This expansion high-\\nlights RAG’s significant practical implications for AI deploy-\\nment, attracting interest from academic and industrial sectors.'}\n",
      "\n",
      "Running BM25-only RAG...\n",
      "{'text': '2\\nFig. 1. Technology tree of RAG research. The stages of involving RAG mainly include pre-training, fine-tuning, and inference. With the emergence of LLMs,\\nresearch on RAG initially focused on leveraging the powerful in context learning abilities of LLMs, primarily concentrating on the inference stage. Subsequent\\nresearch has delved deeper, gradually integrating more with the fine-tuning of LLMs. Researchers have also been exploring ways to enhance language models\\nin the pre-training stage through retrieval-augmented techniques.\\nadvanced RAG, and modular RAG. This review contex-\\ntualizes the broader scope of RAG research within the\\nlandscape of LLMs.\\n• We identify and discuss the central technologies integral\\nto the RAG process, specifically focusing on the aspects\\nof “Retrieval”, “Generation” and “Augmentation”, and\\ndelve into their synergies, elucidating how these com-\\nponents intricately collaborate to form a cohesive and\\neffective RAG framework.\\n• We have summarized the current ass', 'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 2, 'type': 'text', 'chunk_index': 0, 'chunk_count': 4, 'index': 7}, 'bm25_score': 13.414667909651333}\n",
      "{'text': ' the input, enhancing performance in knowledge graph\\nquestion-answering tasks.\\nVIII. CONCLUSION\\nThe summary of this paper, as depicted in Figure 6, empha-\\nsizes RAG’s significant advancement in enhancing the capa-\\nbilities of LLMs by integrating parameterized knowledge from\\nlanguage models with extensive non-parameterized data from\\nexternal knowledge bases. The survey showcases the evolution\\nof RAG technologies and their application on many different\\ntasks. The analysis outlines three developmental paradigms\\nwithin the RAG framework: Naive, Advanced, and Modu-\\nlar RAG, each representing a progressive enhancement over\\nits predecessors. RAG’s technical integration with other AI\\nmethodologies, such as fine-tuning and reinforcement learning,\\nhas further expanded its capabilities. Despite the progress in\\nRAG technology, there are research opportunities to improve\\nits robustness and its ability to handle extended contexts.\\nRAG’s application scope is expanding into multimodal do-\\nmains, adapt', 'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 16, 'type': 'text', 'chunk_index': 3, 'chunk_count': 5, 'index': 95}, 'bm25_score': 10.526791970668636}\n",
      "{'text': 's survey endeavors to\\nfill this gap by mapping out the RAG process and charting\\nits evolution and anticipated future paths, with a focus on the\\nintegration of RAG within LLMs. This paper considers both\\ntechnical paradigms and research methods, summarizing three\\nmain research paradigms from over 100 RAG studies, and\\nanalyzing key technologies in the core stages of “Retrieval,”\\n“Generation,” and “Augmentation.” On the other hand, current\\nresearch tends to focus more on methods, lacking analysis and\\nsummarization of how to evaluate RAG. This paper compre-\\nhensively reviews the downstream tasks, datasets, benchmarks,\\nand evaluation methods applicable to RAG. Overall, this\\npaper sets out to meticulously compile and categorize the\\nfoundational technical concepts, historical progression, and\\nthe spectrum of RAG methodologies and applications that\\nhave emerged post-LLMs. It is designed to equip readers and\\nprofessionals with a detailed and structured understanding of\\nboth large models and RAG.', 'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 1, 'type': 'text', 'chunk_index': 5, 'chunk_count': 7, 'index': 5}, 'bm25_score': 10.019429814679606}\n",
      "{'text': 'ion during\\nretrieval can detrimentally affect RAG’s output quality. This\\nsituation is figuratively referred to as “Misinformation can\\nbe worse than no information at all”. Improving RAG’s\\nresistance to such adversarial or counterfactual inputs is gain-\\ning research momentum and has become a key performance\\nmetric [48], [50], [82]. Cuconasu et al. [54] analyze which\\ntype of documents should be retrieved, evaluate the relevance\\nof the documents to the prompt, their position, and the\\nnumber included in the context. The research findings reveal\\nthat including irrelevant documents can unexpectedly increase\\naccuracy by over 30%, contradicting the initial assumption\\nof reduced quality. These results underscore the importance\\nof developing specialized strategies to integrate retrieval with\\nlanguage generation models, highlighting the need for further\\nresearch and exploration into the robustness of RAG.\\nC. Hybrid Approaches\\nCombining RAG with fine-tuning is emerging as a leading\\nstrategy. Deter', 'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 14, 'type': 'text', 'chunk_index': 4, 'chunk_count': 6, 'index': 85}, 'bm25_score': 8.732536493452027}\n",
      "{'text': 'ten-\\ntion due to their growing prevalence. Among the optimization\\nmethods for LLMs, RAG is often compared with Fine-tuning\\n(FT) and prompt engineering. Each method has distinct charac-\\nteristics as illustrated in Figure 4. We used a quadrant chart to\\nillustrate the differences among three methods in two dimen-\\nsions: external knowledge requirements and model adaption\\nrequirements. Prompt engineering leverages a model’s inherent\\ncapabilities with minimum necessity for external knowledge\\nand model adaption. RAG can be likened to providing a model\\nwith a tailored textbook for information retrieval, ideal for pre-\\ncise information retrieval tasks. In contrast, FT is comparable\\nto a student internalizing knowledge over time, suitable for\\nscenarios requiring replication of specific structures, styles, or\\nformats.\\nRAG excels in dynamic environments by offering real-\\ntime knowledge updates and effective utilization of external\\nknowledge sources with high interpretability. However, it\\ncomes wit', 'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 5, 'type': 'text', 'chunk_index': 4, 'chunk_count': 8, 'index': 25}, 'bm25_score': 8.42914796390345}\n",
      "\n",
      "Running fusion RAG with text and image...\n",
      "Performing fusion retrieval for query: Based on Figure 1 (RAG Technology Tree), how has RAG research evolved over time?\n",
      "Retrieved 222 relevant items (207 text, 15 image captions)\n",
      "Retrieved 5 documents with fusion retrieval\n",
      "{'text': '2\\nFig. 1. Technology tree of RAG research. The stages of involving RAG mainly include pre-training, fine-tuning, and inference. With the emergence of LLMs,\\nresearch on RAG initially focused on leveraging the powerful in context learning abilities of LLMs, primarily concentrating on the inference stage. Subsequent\\nresearch has delved deeper, gradually integrating more with the fine-tuning of LLMs. Researchers have also been exploring ways to enhance language models\\nin the pre-training stage through retrieval-augmented techniques.\\nadvanced RAG, and modular RAG. This review contex-\\ntualizes the broader scope of RAG research within the\\nlandscape of LLMs.\\n• We identify and discuss the central technologies integral\\nto the RAG process, specifically focusing on the aspects\\nof “Retrieval”, “Generation” and “Augmentation”, and\\ndelve into their synergies, elucidating how these com-\\nponents intricately collaborate to form a cohesive and\\neffective RAG framework.\\n• We have summarized the current ass', 'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 2, 'type': 'text', 'chunk_index': 0, 'chunk_count': 4, 'index': 7}, 'vector_score': 0.5740738997929327, 'bm25_score': 13.414667909651333, 'index': 7, 'combined_score': 0.9365178311192153}\n",
      "{'text': ' the input, enhancing performance in knowledge graph\\nquestion-answering tasks.\\nVIII. CONCLUSION\\nThe summary of this paper, as depicted in Figure 6, empha-\\nsizes RAG’s significant advancement in enhancing the capa-\\nbilities of LLMs by integrating parameterized knowledge from\\nlanguage models with extensive non-parameterized data from\\nexternal knowledge bases. The survey showcases the evolution\\nof RAG technologies and their application on many different\\ntasks. The analysis outlines three developmental paradigms\\nwithin the RAG framework: Naive, Advanced, and Modu-\\nlar RAG, each representing a progressive enhancement over\\nits predecessors. RAG’s technical integration with other AI\\nmethodologies, such as fine-tuning and reinforcement learning,\\nhas further expanded its capabilities. Despite the progress in\\nRAG technology, there are research opportunities to improve\\nits robustness and its ability to handle extended contexts.\\nRAG’s application scope is expanding into multimodal do-\\nmains, adapt', 'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 16, 'type': 'text', 'chunk_index': 3, 'chunk_count': 5, 'index': 95}, 'vector_score': 0.5634364685260677, 'bm25_score': 10.526791970668636, 'index': 95, 'combined_score': 0.8207905346410544}\n",
      "{'text': 's survey endeavors to\\nfill this gap by mapping out the RAG process and charting\\nits evolution and anticipated future paths, with a focus on the\\nintegration of RAG within LLMs. This paper considers both\\ntechnical paradigms and research methods, summarizing three\\nmain research paradigms from over 100 RAG studies, and\\nanalyzing key technologies in the core stages of “Retrieval,”\\n“Generation,” and “Augmentation.” On the other hand, current\\nresearch tends to focus more on methods, lacking analysis and\\nsummarization of how to evaluate RAG. This paper compre-\\nhensively reviews the downstream tasks, datasets, benchmarks,\\nand evaluation methods applicable to RAG. Overall, this\\npaper sets out to meticulously compile and categorize the\\nfoundational technical concepts, historical progression, and\\nthe spectrum of RAG methodologies and applications that\\nhave emerged post-LLMs. It is designed to equip readers and\\nprofessionals with a detailed and structured understanding of\\nboth large models and RAG.', 'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 1, 'type': 'text', 'chunk_index': 5, 'chunk_count': 7, 'index': 5}, 'vector_score': 0.5597076247136823, 'bm25_score': 10.019429814679606, 'index': 5, 'combined_score': 0.7990444498638086}\n",
      "{'text': 'ten-\\ntion due to their growing prevalence. Among the optimization\\nmethods for LLMs, RAG is often compared with Fine-tuning\\n(FT) and prompt engineering. Each method has distinct charac-\\nteristics as illustrated in Figure 4. We used a quadrant chart to\\nillustrate the differences among three methods in two dimen-\\nsions: external knowledge requirements and model adaption\\nrequirements. Prompt engineering leverages a model’s inherent\\ncapabilities with minimum necessity for external knowledge\\nand model adaption. RAG can be likened to providing a model\\nwith a tailored textbook for information retrieval, ideal for pre-\\ncise information retrieval tasks. In contrast, FT is comparable\\nto a student internalizing knowledge over time, suitable for\\nscenarios requiring replication of specific structures, styles, or\\nformats.\\nRAG excels in dynamic environments by offering real-\\ntime knowledge updates and effective utilization of external\\nknowledge sources with high interpretability. However, it\\ncomes wit', 'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 5, 'type': 'text', 'chunk_index': 4, 'chunk_count': 8, 'index': 25}, 'vector_score': 0.49248066419085923, 'bm25_score': 8.42914796390345, 'index': 25, 'combined_score': 0.6886520089825274}\n",
      "{'text': 'ion during\\nretrieval can detrimentally affect RAG’s output quality. This\\nsituation is figuratively referred to as “Misinformation can\\nbe worse than no information at all”. Improving RAG’s\\nresistance to such adversarial or counterfactual inputs is gain-\\ning research momentum and has become a key performance\\nmetric [48], [50], [82]. Cuconasu et al. [54] analyze which\\ntype of documents should be retrieved, evaluate the relevance\\nof the documents to the prompt, their position, and the\\nnumber included in the context. The research findings reveal\\nthat including irrelevant documents can unexpectedly increase\\naccuracy by over 30%, contradicting the initial assumption\\nof reduced quality. These results underscore the importance\\nof developing specialized strategies to integrate retrieval with\\nlanguage generation models, highlighting the need for further\\nresearch and exploration into the robustness of RAG.\\nC. Hybrid Approaches\\nCombining RAG with fine-tuning is emerging as a leading\\nstrategy. Deter', 'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 14, 'type': 'text', 'chunk_index': 4, 'chunk_count': 6, 'index': 85}, 'vector_score': 0.4226657925493383, 'bm25_score': 8.732536493452027, 'index': 85, 'combined_score': 0.6468738383082242}\n",
      "\n",
      "Comparing responses...\n",
      "\n",
      "=== Vector-based Response with out image ===\n",
      "Based on Figure 1 (RAG Technology Tree), the evolution of RAG research can be summarized as follows:\n",
      "\n",
      "1. **Initial Focus on Inference**: The research initially concentrated on leveraging the powerful in-context learning abilities of Large Language Models (LLMs), primarily focusing on the inference stage. This stage is characterized by the Naive RAG paradigm, which represents the earliest methodology.\n",
      "\n",
      "2. **Integration with Fine-Tuning**: As research progressed, there was a deeper integration with the fine-tuning of LLMs. This development corresponds to the Advanced RAG stage, which emerged as a response to the limitations of Naive RAG.\n",
      "\n",
      "3. **Enhancements in Pre-Training**: Researchers have also explored ways to enhance language models in the pre-training stage through retrieval-augmented techniques. This represents a more sophisticated approach to improving the capabilities of RAG.\n",
      "\n",
      "4. **Developmental Paradigms**: The evolution of RAG is categorized into three stages: Naive RAG, Advanced RAG, and Modular RAG. Each stage represents a progressive enhancement over its predecessors, addressing specific shortcomings and expanding the capabilities of RAG.\n",
      "\n",
      "5. **Expansion into Multimodal Domains**: RAG has transcended its initial text-based confines and is now embracing a diverse array of modal data, including images and videos. This expansion into multimodal domains highlights the significant practical implications of RAG for AI deployment.\n",
      "\n",
      "Overall, the RAG research has evolved from a focus on inference to a more integrated approach involving fine-tuning and pre-training, with an expanding scope into multimodal applications. This evolution reflects continuous advancements in the technology stack and the mutual growth of RAG models and their capabilities.\n",
      "\n",
      "=== Vector-based Response with image ===\n",
      "Based on Figure 1 (RAG Technology Tree), the evolution of RAG (Retrieval-Augmented Generation) research can be understood through several key stages and developments:\n",
      "\n",
      "1. **Initial Focus on Inference**: The early research on RAG concentrated on leveraging the powerful in-context learning abilities of Large Language Models (LLMs), primarily focusing on the inference stage. This stage is characterized by the initial attempts to integrate retrieval mechanisms with language models to enhance their performance.\n",
      "\n",
      "2. **Integration with Fine-Tuning**: As research progressed, there was a deeper integration with the fine-tuning of LLMs. This involved adapting and optimizing language models to better utilize retrieved information, thereby improving their generative capabilities.\n",
      "\n",
      "3. **Enhancements in Pre-Training**: Researchers have also explored ways to enhance language models during the pre-training stage using retrieval-augmented techniques. This represents a more foundational approach to integrating retrieval mechanisms, aiming to improve the overall learning process of the models.\n",
      "\n",
      "4. **Developmental Paradigms**: The evolution of RAG is categorized into three stages: Naive RAG, Advanced RAG, and Modular RAG. Each stage represents a progressive enhancement over its predecessors, addressing specific limitations and expanding the capabilities of RAG systems.\n",
      "\n",
      "5. **Expansion into Multimodal Domains**: RAG has transcended its initial text-based confines and is now being applied to multimodal data, including images, videos, and code. This expansion indicates a significant broadening of RAG's application scope and practical implications for AI deployment.\n",
      "\n",
      "Overall, the RAG research has evolved from focusing on inference to a more integrated approach involving fine-tuning and pre-training, with a growing emphasis on multimodal applications. This evolution reflects the continuous advancements in RAG technologies and their integration with other AI methodologies.\n",
      "\n",
      "=== BM25 Response ===\n",
      "Based on Figure 1 (RAG Technology Tree) and the accompanying text, the evolution of RAG (Retrieval-Augmented Generation) research can be summarized as follows:\n",
      "\n",
      "1. **Initial Focus on Inference**: RAG research initially concentrated on leveraging the powerful in-context learning abilities of Large Language Models (LLMs), primarily focusing on the inference stage. This stage involves using pre-trained models to generate responses based on retrieved information.\n",
      "\n",
      "2. **Integration with Fine-Tuning**: As research progressed, there was a deeper integration with the fine-tuning of LLMs. This involves adjusting the pre-trained models to better handle specific tasks or datasets, enhancing their performance in generating relevant and accurate outputs.\n",
      "\n",
      "3. **Enhancements in Pre-Training**: Researchers have also explored ways to enhance language models during the pre-training stage using retrieval-augmented techniques. This involves incorporating external knowledge sources during the initial training phase of the models to improve their foundational understanding and capabilities.\n",
      "\n",
      "4. **Developmental Paradigms**: The evolution of RAG technologies is categorized into three developmental paradigms: Naive, Advanced, and Modular RAG. Each paradigm represents a progressive enhancement over its predecessors, indicating a structured evolution in the approach to integrating retrieval with generation.\n",
      "\n",
      "5. **Technical Integration and Expansion**: RAG has been technically integrated with other AI methodologies, such as fine-tuning and reinforcement learning, which has expanded its capabilities. The application scope of RAG is also expanding into multimodal domains, indicating its growing versatility and applicability.\n",
      "\n",
      "Overall, the evolution of RAG research reflects a shift from a focus on inference to a more comprehensive integration across all stages of model development, with ongoing advancements in methodologies and applications.\n",
      "\n",
      "=== Fusion Response ===\n",
      "Based on Figure 1 (RAG Technology Tree) and the accompanying text, the evolution of RAG (Retrieval-Augmented Generation) research can be summarized as follows:\n",
      "\n",
      "1. **Initial Focus on Inference**: The early stages of RAG research concentrated on leveraging the powerful in-context learning abilities of Large Language Models (LLMs), primarily focusing on the inference stage. This involved using LLMs to enhance the generation of responses by integrating retrieval mechanisms.\n",
      "\n",
      "2. **Integration with Fine-Tuning**: As research progressed, there was a deeper integration of RAG with the fine-tuning of LLMs. This stage involved adapting and optimizing LLMs to work more effectively with retrieval-augmented techniques, thereby improving their performance on specific tasks.\n",
      "\n",
      "3. **Enhancements in Pre-Training**: Researchers have also explored ways to enhance language models during the pre-training stage using retrieval-augmented techniques. This involves incorporating external knowledge bases into the pre-training process to enrich the model's understanding and capabilities.\n",
      "\n",
      "4. **Developmental Paradigms**: The evolution of RAG technologies is categorized into three developmental paradigms: Naive, Advanced, and Modular RAG. Each paradigm represents a progressive enhancement over its predecessors, indicating a structured evolution in the complexity and capability of RAG systems.\n",
      "\n",
      "5. **Technical Integration and Expansion**: RAG's integration with other AI methodologies, such as fine-tuning and reinforcement learning, has expanded its capabilities. The application scope of RAG is also expanding into multimodal domains, indicating its growing versatility and applicability.\n",
      "\n",
      "Overall, the evolution of RAG research reflects a shift from initial inference-focused applications to more integrated and sophisticated approaches that enhance LLMs across various stages of their development and application.\n",
      "\n",
      "=== Comparison ===\n",
      "### Comparison of Responses\n",
      "\n",
      "#### 1. Vector-based Retrieval\n",
      "\n",
      "- **Relevance to the Query**: \n",
      "  - The response is highly relevant, providing a detailed chronological evolution of RAG research.\n",
      "  - It aligns well with the query by outlining the stages of RAG development.\n",
      "\n",
      "- **Factual Correctness**: \n",
      "  - The response accurately describes the stages of RAG evolution, including Naive, Advanced, and Modular RAG.\n",
      "  - It correctly identifies the shift from inference to more integrated approaches.\n",
      "\n",
      "- **Comprehensiveness**: \n",
      "  - The response is comprehensive, covering all major stages and developments in RAG research.\n",
      "  - It includes the expansion into multimodal domains, which adds depth to the explanation.\n",
      "\n",
      "- **Clarity and Coherence**: \n",
      "  - The response is clear and logically structured, making it easy to follow.\n",
      "  - Each stage of evolution is distinctly outlined, contributing to coherence.\n",
      "\n",
      "#### 2. Vector-based Retrieval with Images\n",
      "\n",
      "- **Relevance to the Query**: \n",
      "  - Similar to the vector-based response, it is highly relevant and follows the query closely.\n",
      "  - It provides a detailed account of RAG's evolution.\n",
      "\n",
      "- **Factual Correctness**: \n",
      "  - The response is factually correct, mirroring the stages and developments mentioned in the vector-based response.\n",
      "  - It accurately describes the integration of retrieval mechanisms and multimodal expansion.\n",
      "\n",
      "- **Comprehensiveness**: \n",
      "  - The response is comprehensive, covering all key aspects of RAG's evolution.\n",
      "  - It includes the application to multimodal data, enhancing the depth of the response.\n",
      "\n",
      "- **Clarity and Coherence**: \n",
      "  - The response is clear and well-organized, similar to the vector-based response.\n",
      "  - The use of structured points aids in maintaining coherence.\n",
      "\n",
      "#### 3. BM25 Keyword Retrieval\n",
      "\n",
      "- **Relevance to the Query**: \n",
      "  - The response is relevant but slightly less detailed compared to the vector-based responses.\n",
      "  - It addresses the main points but lacks some depth in explanation.\n",
      "\n",
      "- **Factual Correctness**: \n",
      "  - The response is factually correct, covering the main stages of RAG evolution.\n",
      "  - It accurately mentions the integration with other AI methodologies.\n",
      "\n",
      "- **Comprehensiveness**: \n",
      "  - The response is less comprehensive, missing some nuances such as the specific impact of multimodal expansion.\n",
      "  - It provides a broad overview but lacks detailed insights.\n",
      "\n",
      "- **Clarity and Coherence**: \n",
      "  - The response is clear but not as detailed or structured as the vector-based responses.\n",
      "  - It could benefit from more explicit structuring of the stages.\n",
      "\n",
      "#### 4. Fusion Retrieval\n",
      "\n",
      "- **Relevance to the Query**: \n",
      "  - The response is highly relevant, effectively combining elements from both vector and keyword approaches.\n",
      "  - It provides a thorough overview of RAG's evolution.\n",
      "\n",
      "- **Factual Correctness**: \n",
      "  - The response is factually accurate, covering all key stages and developments.\n",
      "  - It correctly identifies the integration with other AI methodologies and multimodal expansion.\n",
      "\n",
      "- **Comprehensiveness**: \n",
      "  - The response is comprehensive, capturing the essence of RAG's evolution.\n",
      "  - It includes detailed insights into the developmental paradigms and technical integration.\n",
      "\n",
      "- **Clarity and Coherence**: \n",
      "  - The response is clear and well-structured, making it easy to follow.\n",
      "  - It effectively combines information from both retrieval approaches, enhancing coherence.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "**Best Performing Approach: Fusion Retrieval**\n",
      "\n",
      "- **Strengths**: \n",
      "  - The fusion retrieval approach performed best due to its comprehensive coverage and clear structure.\n",
      "  - It effectively combines the strengths of both vector and keyword approaches, providing a detailed and coherent response.\n",
      "  - The response is highly relevant, factually correct, and includes insights into the integration and expansion of RAG technologies.\n",
      "\n",
      "- **Weaknesses**: \n",
      "  - There are no significant weaknesses in the fusion response for this query. It successfully addresses all aspects of the query with clarity and depth.\n",
      "\n",
      "Overall, the fusion retrieval approach offers the most balanced and informative response, making it the best choice for this query.\n",
      "\n",
      "\n",
      "=== Evaluating Query 3/3 ===\n",
      "Query: What is a primary challenge that LLMs face which RAG is designed to solve?\n",
      "\n",
      "=== Comparing retrieval methods for query: What is a primary challenge that LLMs face which RAG is designed to solve? ===\n",
      "\n",
      "\n",
      "Running vector-only RAG with text only...\n",
      "Retrieved 5 documents using vector search\n",
      "{'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 2, 'type': 'text', 'chunk_index': 3, 'chunk_count': 4, 'index': 10}, 'similarity': 0.6048435676260743, 'text': 'les related\\nto the user’s query. These articles, combined with the original\\nquestion, form a comprehensive prompt that empowers LLMs\\nto generate a well-informed answer.\\nThe RAG research paradigm is continuously evolving, and\\nwe categorize it into three stages: Naive RAG, Advanced\\nRAG, and Modular RAG, as showed in Figure 3. Despite\\nRAG method are cost-effective and surpass the performance\\nof the native LLM, they also exhibit several limitations.\\nThe development of Advanced RAG and Modular RAG is\\na response to these specific shortcomings in Naive RAG.\\nA. Naive RAG\\nThe Naive RAG research paradigm represents the earli-\\nest methodology, which gained prominence shortly after the'}\n",
      "{'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 14, 'type': 'text', 'chunk_index': 2, 'chunk_count': 6, 'index': 83}, 'similarity': 0.5444271049991761, 'text': 'epth research.This\\nchapter will mainly introduce the current challenges and future\\nresearch directions faced by RAG.\\nA. RAG vs Long Context\\nWith the deepening of related research, the context of LLMs\\nis continuously expanding [170]–[172]. Presently, LLMs can\\neffortlessly manage contexts exceeding 200,000 tokens 9. This\\ncapability signifies that long-document question answering,\\npreviously reliant on RAG, can now incorporate the entire\\ndocument directly into the prompt. This has also sparked\\ndiscussions on whether RAG is still necessary when LLMs\\n8https://www.trulens.org/trulens eval/core concepts rag triad/\\n9https://kimi.moonshot.cn\\nare not constrained by context. In fact, RAG still plays an\\nirreplaceable role. On one hand, providing LLMs with a\\nlarge amount of context at once will significantly impact its\\ninference speed, while chunked retrieval and on-demand input\\ncan significantly improve operational efficiency. On the other\\nhand, RAG-based generation can quickly locate the original'}\n",
      "{'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 14, 'type': 'text', 'chunk_index': 1, 'chunk_count': 6, 'index': 82}, 'similarity': 0.5442704009129569, 'text': ' benchmark tests and tools have been proposed\\nto facilitate the evaluation of RAG.These instruments furnish\\nquantitative metrics that not only gauge RAG model perfor-\\nmance but also enhance comprehension of the model’s capabil-\\nities across various evaluation aspects. Prominent benchmarks\\nsuch as RGB, RECALL and CRUD\\n[167]–[169] focus on\\nappraising the essential abilities of RAG models. Concur-\\nrently, state-of-the-art automated tools like RAGAS [164],\\nARES [165], and TruLens8 employ LLMs to adjudicate the\\nquality scores. These tools and benchmarks collectively form\\na robust framework for the systematic evaluation of RAG\\nmodels, as summarized in Table IV.\\nVII. DISCUSSION AND FUTURE PROSPECTS\\nDespite the considerable progress in RAG technology, sev-\\neral challenges persist that warrant in-depth research.This\\nchapter will mainly introduce the current challenges and future\\nresearch directions faced by RAG.\\nA. RAG vs Long Context\\nWith the deepening of related research, the context of LLMs\\n'}\n",
      "{'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 15, 'type': 'text', 'chunk_index': 3, 'chunk_count': 5, 'index': 90}, 'similarity': 0.5348626727051038, 'text': 'caling/prize\\ninadvertent disclosure of document sources or metadata by\\nLLMs—are critical engineering challenges that remain to be\\naddressed [175].\\nThe development of the RAG ecosystem is greatly impacted\\nby the progression of its technical stack. Key tools like\\nLangChain and LLamaIndex have quickly gained popularity\\nwith the emergence of ChatGPT, providing extensive RAG-\\nrelated APIs and becoming essential in the realm of LLMs.The\\nemerging technology stack, while not as rich in features as\\nLangChain and LLamaIndex, stands out through its specialized\\nproducts. For example, Flowise AI prioritizes a low-code\\napproach, allowing users to deploy AI applications, including\\nRAG, through a user-friendly drag-and-drop interface. Other\\ntechnologies like HayStack, Meltano, and Cohere Coral are\\nalso gaining attention for their unique contributions to the field.\\nIn addition to AI-focused vendors, traditional software and\\ncloud service providers are expanding their offerings to include\\nRAG-centric se'}\n",
      "{'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 5, 'type': 'text', 'chunk_index': 4, 'chunk_count': 8, 'index': 25}, 'similarity': 0.5336636955804218, 'text': 'ten-\\ntion due to their growing prevalence. Among the optimization\\nmethods for LLMs, RAG is often compared with Fine-tuning\\n(FT) and prompt engineering. Each method has distinct charac-\\nteristics as illustrated in Figure 4. We used a quadrant chart to\\nillustrate the differences among three methods in two dimen-\\nsions: external knowledge requirements and model adaption\\nrequirements. Prompt engineering leverages a model’s inherent\\ncapabilities with minimum necessity for external knowledge\\nand model adaption. RAG can be likened to providing a model\\nwith a tailored textbook for information retrieval, ideal for pre-\\ncise information retrieval tasks. In contrast, FT is comparable\\nto a student internalizing knowledge over time, suitable for\\nscenarios requiring replication of specific structures, styles, or\\nformats.\\nRAG excels in dynamic environments by offering real-\\ntime knowledge updates and effective utilization of external\\nknowledge sources with high interpretability. However, it\\ncomes wit'}\n",
      "\n",
      "Running vector-only RAG with text and image captions...\n",
      "Retrieved 5 documents using vector search\n",
      "{'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 2, 'type': 'text', 'chunk_index': 3, 'chunk_count': 4, 'index': 10}, 'similarity': 0.6042316305793801, 'text': 'les related\\nto the user’s query. These articles, combined with the original\\nquestion, form a comprehensive prompt that empowers LLMs\\nto generate a well-informed answer.\\nThe RAG research paradigm is continuously evolving, and\\nwe categorize it into three stages: Naive RAG, Advanced\\nRAG, and Modular RAG, as showed in Figure 3. Despite\\nRAG method are cost-effective and surpass the performance\\nof the native LLM, they also exhibit several limitations.\\nThe development of Advanced RAG and Modular RAG is\\na response to these specific shortcomings in Naive RAG.\\nA. Naive RAG\\nThe Naive RAG research paradigm represents the earli-\\nest methodology, which gained prominence shortly after the'}\n",
      "{'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 14, 'type': 'text', 'chunk_index': 1, 'chunk_count': 6, 'index': 82}, 'similarity': 0.5446351278566963, 'text': ' benchmark tests and tools have been proposed\\nto facilitate the evaluation of RAG.These instruments furnish\\nquantitative metrics that not only gauge RAG model perfor-\\nmance but also enhance comprehension of the model’s capabil-\\nities across various evaluation aspects. Prominent benchmarks\\nsuch as RGB, RECALL and CRUD\\n[167]–[169] focus on\\nappraising the essential abilities of RAG models. Concur-\\nrently, state-of-the-art automated tools like RAGAS [164],\\nARES [165], and TruLens8 employ LLMs to adjudicate the\\nquality scores. These tools and benchmarks collectively form\\na robust framework for the systematic evaluation of RAG\\nmodels, as summarized in Table IV.\\nVII. DISCUSSION AND FUTURE PROSPECTS\\nDespite the considerable progress in RAG technology, sev-\\neral challenges persist that warrant in-depth research.This\\nchapter will mainly introduce the current challenges and future\\nresearch directions faced by RAG.\\nA. RAG vs Long Context\\nWith the deepening of related research, the context of LLMs\\n'}\n",
      "{'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 14, 'type': 'text', 'chunk_index': 2, 'chunk_count': 6, 'index': 83}, 'similarity': 0.5445398079198376, 'text': 'epth research.This\\nchapter will mainly introduce the current challenges and future\\nresearch directions faced by RAG.\\nA. RAG vs Long Context\\nWith the deepening of related research, the context of LLMs\\nis continuously expanding [170]–[172]. Presently, LLMs can\\neffortlessly manage contexts exceeding 200,000 tokens 9. This\\ncapability signifies that long-document question answering,\\npreviously reliant on RAG, can now incorporate the entire\\ndocument directly into the prompt. This has also sparked\\ndiscussions on whether RAG is still necessary when LLMs\\n8https://www.trulens.org/trulens eval/core concepts rag triad/\\n9https://kimi.moonshot.cn\\nare not constrained by context. In fact, RAG still plays an\\nirreplaceable role. On one hand, providing LLMs with a\\nlarge amount of context at once will significantly impact its\\ninference speed, while chunked retrieval and on-demand input\\ncan significantly improve operational efficiency. On the other\\nhand, RAG-based generation can quickly locate the original'}\n",
      "{'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 15, 'type': 'text', 'chunk_index': 3, 'chunk_count': 5, 'index': 90}, 'similarity': 0.5346747411340212, 'text': 'caling/prize\\ninadvertent disclosure of document sources or metadata by\\nLLMs—are critical engineering challenges that remain to be\\naddressed [175].\\nThe development of the RAG ecosystem is greatly impacted\\nby the progression of its technical stack. Key tools like\\nLangChain and LLamaIndex have quickly gained popularity\\nwith the emergence of ChatGPT, providing extensive RAG-\\nrelated APIs and becoming essential in the realm of LLMs.The\\nemerging technology stack, while not as rich in features as\\nLangChain and LLamaIndex, stands out through its specialized\\nproducts. For example, Flowise AI prioritizes a low-code\\napproach, allowing users to deploy AI applications, including\\nRAG, through a user-friendly drag-and-drop interface. Other\\ntechnologies like HayStack, Meltano, and Cohere Coral are\\nalso gaining attention for their unique contributions to the field.\\nIn addition to AI-focused vendors, traditional software and\\ncloud service providers are expanding their offerings to include\\nRAG-centric se'}\n",
      "{'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 5, 'type': 'text', 'chunk_index': 4, 'chunk_count': 8, 'index': 25}, 'similarity': 0.5328598485431957, 'text': 'ten-\\ntion due to their growing prevalence. Among the optimization\\nmethods for LLMs, RAG is often compared with Fine-tuning\\n(FT) and prompt engineering. Each method has distinct charac-\\nteristics as illustrated in Figure 4. We used a quadrant chart to\\nillustrate the differences among three methods in two dimen-\\nsions: external knowledge requirements and model adaption\\nrequirements. Prompt engineering leverages a model’s inherent\\ncapabilities with minimum necessity for external knowledge\\nand model adaption. RAG can be likened to providing a model\\nwith a tailored textbook for information retrieval, ideal for pre-\\ncise information retrieval tasks. In contrast, FT is comparable\\nto a student internalizing knowledge over time, suitable for\\nscenarios requiring replication of specific structures, styles, or\\nformats.\\nRAG excels in dynamic environments by offering real-\\ntime knowledge updates and effective utilization of external\\nknowledge sources with high interpretability. However, it\\ncomes wit'}\n",
      "\n",
      "Running BM25-only RAG...\n",
      "{'text': 'with the search outcomes.\\nTo address specific data scenarios, recursive retrieval and\\nmulti-hop retrieval techniques are utilized together. Recursive\\nretrieval involves a structured index to process and retrieve\\ndata in a hierarchical manner, which may include summarizing\\nsections of a document or lengthy PDF before performing a\\nretrieval based on this summary. Subsequently, a secondary\\nretrieval within the document refines the search, embodying\\nthe recursive nature of the process. In contrast, multi-hop\\nretrieval is designed to delve deeper into graph-structured data\\nsources, extracting interconnected information [106].\\nC. Adaptive Retrieval\\nAdaptive retrieval methods, exemplified by Flare [24] and\\nSelf-RAG [25], refine the RAG framework by enabling LLMs\\nto actively determine the optimal moments and content for\\nretrieval, thus enhancing the efficiency and relevance of the\\ninformation sourced.\\nThese methods are part of a broader trend wherein\\nLLMs employ active judgment in their operat', 'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 11, 'type': 'text', 'chunk_index': 3, 'chunk_count': 6, 'index': 67}, 'bm25_score': 13.550221362064828}\n",
      "{'text': '7\\nFig. 4. RAG compared with other model optimization methods in the aspects of “External Knowledge Required” and “Model Adaption Required”. Prompt\\nEngineering requires low modifications to the model and external knowledge, focusing on harnessing the capabilities of LLMs themselves. Fine-tuning, on\\nthe other hand, involves further training the model. In the early stages of RAG (Naive RAG), there is a low demand for model modifications. As research\\nprogresses, Modular RAG has become more integrated with fine-tuning techniques.\\nUnstructured Data, such as text, is the most widely used\\nretrieval source, which are mainly gathered from corpus. For\\nopen-domain question-answering (ODQA) tasks, the primary\\nretrieval sources are Wikipedia Dump with the current major\\nversions including HotpotQA 4 (1st October , 2017), DPR5 (20\\nDecember, 2018). In addition to encyclopedic data, common\\nunstructured data includes cross-lingual text [19] and domain-\\nspecific data (such as medical [67]and legal domains', 'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 7, 'type': 'text', 'chunk_index': 0, 'chunk_count': 6, 'index': 34}, 'bm25_score': 13.094026894480077}\n",
      "{'text': 'les related\\nto the user’s query. These articles, combined with the original\\nquestion, form a comprehensive prompt that empowers LLMs\\nto generate a well-informed answer.\\nThe RAG research paradigm is continuously evolving, and\\nwe categorize it into three stages: Naive RAG, Advanced\\nRAG, and Modular RAG, as showed in Figure 3. Despite\\nRAG method are cost-effective and surpass the performance\\nof the native LLM, they also exhibit several limitations.\\nThe development of Advanced RAG and Modular RAG is\\na response to these specific shortcomings in Naive RAG.\\nA. Naive RAG\\nThe Naive RAG research paradigm represents the earli-\\nest methodology, which gained prominence shortly after the', 'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 2, 'type': 'text', 'chunk_index': 3, 'chunk_count': 4, 'index': 10}, 'bm25_score': 12.539791444647745}\n",
      "{'text': 'he posed query and selected documents are\\nsynthesized into a coherent prompt to which a large language\\nmodel is tasked with formulating a response. The model’s\\napproach to answering may vary depending on task-specific\\ncriteria, allowing it to either draw upon its inherent parametric\\nknowledge or restrict its responses to the information con-\\ntained within the provided documents. In cases of ongoing\\ndialogues, any existing conversational history can be integrated\\ninto the prompt, enabling the model to engage in multi-turn\\ndialogue interactions effectively.\\nHowever, Naive RAG encounters notable drawbacks:\\nRetrieval Challenges. The retrieval phase often struggles\\nwith precision and recall, leading to the selection of misaligned\\nor irrelevant chunks, and the missing of crucial information.\\nGeneration Difficulties. In generating responses, the model\\nmay face the issue of hallucination, where it produces con-\\ntent not supported by the retrieved context. This phase can\\nalso suffer from irrele', 'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 3, 'type': 'text', 'chunk_index': 2, 'chunk_count': 5, 'index': 13}, 'bm25_score': 11.991849364462007}\n",
      "{'text': 'ining attention for their unique contributions to the field.\\nIn addition to AI-focused vendors, traditional software and\\ncloud service providers are expanding their offerings to include\\nRAG-centric services. Weaviate’s Verba 11 is designed for\\npersonal assistant applications, while Amazon’s Kendra\\n12\\noffers intelligent enterprise search services, enabling users to\\nbrowse various content repositories using built-in connectors.\\nIn the development of RAG technology, there is a clear\\ntrend towards different specialization directions, such as: 1)\\nCustomization - tailoring RAG to meet specific requirements.\\n2) Simplification - making RAG easier to use to reduce the\\n11https://github.com/weaviate/Verba\\n12https://aws.amazon.com/cn/kendra/', 'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 15, 'type': 'text', 'chunk_index': 4, 'chunk_count': 5, 'index': 91}, 'bm25_score': 11.827296548062629}\n",
      "\n",
      "Running fusion RAG with text and image...\n",
      "Performing fusion retrieval for query: What is a primary challenge that LLMs face which RAG is designed to solve?\n",
      "Retrieved 222 relevant items (211 text, 11 image captions)\n",
      "Retrieved 5 documents with fusion retrieval\n",
      "{'text': 'les related\\nto the user’s query. These articles, combined with the original\\nquestion, form a comprehensive prompt that empowers LLMs\\nto generate a well-informed answer.\\nThe RAG research paradigm is continuously evolving, and\\nwe categorize it into three stages: Naive RAG, Advanced\\nRAG, and Modular RAG, as showed in Figure 3. Despite\\nRAG method are cost-effective and surpass the performance\\nof the native LLM, they also exhibit several limitations.\\nThe development of Advanced RAG and Modular RAG is\\na response to these specific shortcomings in Naive RAG.\\nA. Naive RAG\\nThe Naive RAG research paradigm represents the earli-\\nest methodology, which gained prominence shortly after the', 'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 2, 'type': 'text', 'chunk_index': 3, 'chunk_count': 4, 'index': 10}, 'vector_score': 0.6041882154483388, 'bm25_score': 12.539791444647745, 'index': 10, 'combined_score': 0.9627153636850645}\n",
      "{'text': '7\\nFig. 4. RAG compared with other model optimization methods in the aspects of “External Knowledge Required” and “Model Adaption Required”. Prompt\\nEngineering requires low modifications to the model and external knowledge, focusing on harnessing the capabilities of LLMs themselves. Fine-tuning, on\\nthe other hand, involves further training the model. In the early stages of RAG (Naive RAG), there is a low demand for model modifications. As research\\nprogresses, Modular RAG has become more integrated with fine-tuning techniques.\\nUnstructured Data, such as text, is the most widely used\\nretrieval source, which are mainly gathered from corpus. For\\nopen-domain question-answering (ODQA) tasks, the primary\\nretrieval sources are Wikipedia Dump with the current major\\nversions including HotpotQA 4 (1st October , 2017), DPR5 (20\\nDecember, 2018). In addition to encyclopedic data, common\\nunstructured data includes cross-lingual text [19] and domain-\\nspecific data (such as medical [67]and legal domains', 'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 7, 'type': 'text', 'chunk_index': 0, 'chunk_count': 6, 'index': 34}, 'vector_score': 0.43929135097460353, 'bm25_score': 13.094026894480077, 'index': 34, 'combined_score': 0.8467050210047211}\n",
      "{'text': 'with the search outcomes.\\nTo address specific data scenarios, recursive retrieval and\\nmulti-hop retrieval techniques are utilized together. Recursive\\nretrieval involves a structured index to process and retrieve\\ndata in a hierarchical manner, which may include summarizing\\nsections of a document or lengthy PDF before performing a\\nretrieval based on this summary. Subsequently, a secondary\\nretrieval within the document refines the search, embodying\\nthe recursive nature of the process. In contrast, multi-hop\\nretrieval is designed to delve deeper into graph-structured data\\nsources, extracting interconnected information [106].\\nC. Adaptive Retrieval\\nAdaptive retrieval methods, exemplified by Flare [24] and\\nSelf-RAG [25], refine the RAG framework by enabling LLMs\\nto actively determine the optimal moments and content for\\nretrieval, thus enhancing the efficiency and relevance of the\\ninformation sourced.\\nThese methods are part of a broader trend wherein\\nLLMs employ active judgment in their operat', 'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 11, 'type': 'text', 'chunk_index': 3, 'chunk_count': 6, 'index': 67}, 'vector_score': 0.3881266701656089, 'bm25_score': 13.550221362064828, 'index': 67, 'combined_score': 0.821196816961892}\n",
      "{'text': 'he posed query and selected documents are\\nsynthesized into a coherent prompt to which a large language\\nmodel is tasked with formulating a response. The model’s\\napproach to answering may vary depending on task-specific\\ncriteria, allowing it to either draw upon its inherent parametric\\nknowledge or restrict its responses to the information con-\\ntained within the provided documents. In cases of ongoing\\ndialogues, any existing conversational history can be integrated\\ninto the prompt, enabling the model to engage in multi-turn\\ndialogue interactions effectively.\\nHowever, Naive RAG encounters notable drawbacks:\\nRetrieval Challenges. The retrieval phase often struggles\\nwith precision and recall, leading to the selection of misaligned\\nor irrelevant chunks, and the missing of crucial information.\\nGeneration Difficulties. In generating responses, the model\\nmay face the issue of hallucination, where it produces con-\\ntent not supported by the retrieved context. This phase can\\nalso suffer from irrele', 'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 3, 'type': 'text', 'chunk_index': 2, 'chunk_count': 5, 'index': 13}, 'vector_score': 0.44071719844967894, 'bm25_score': 11.991849364462007, 'index': 13, 'combined_score': 0.8072148974820077}\n",
      "{'text': 'ining attention for their unique contributions to the field.\\nIn addition to AI-focused vendors, traditional software and\\ncloud service providers are expanding their offerings to include\\nRAG-centric services. Weaviate’s Verba 11 is designed for\\npersonal assistant applications, while Amazon’s Kendra\\n12\\noffers intelligent enterprise search services, enabling users to\\nbrowse various content repositories using built-in connectors.\\nIn the development of RAG technology, there is a clear\\ntrend towards different specialization directions, such as: 1)\\nCustomization - tailoring RAG to meet specific requirements.\\n2) Simplification - making RAG easier to use to reduce the\\n11https://github.com/weaviate/Verba\\n12https://aws.amazon.com/cn/kendra/', 'metadata': {'source': '/Users/aifora/Desktop/personal_laptop/learning_practice/git_projects/question_answer_chatbot/data/RAG_white_papers_articles.pdf', 'page': 15, 'type': 'text', 'chunk_index': 4, 'chunk_count': 5, 'index': 91}, 'vector_score': 0.34795862817990697, 'bm25_score': 11.827296548062629, 'index': 91, 'combined_score': 0.7243799628813868}\n",
      "\n",
      "Comparing responses...\n",
      "\n",
      "=== Vector-based Response with out image ===\n",
      "A primary challenge that Large Language Models (LLMs) face, which Retrieval-Augmented Generation (RAG) is designed to solve, is the efficient handling of large contexts. While LLMs can manage extensive contexts, providing them with a large amount of context at once can significantly impact their inference speed. RAG addresses this by using chunked retrieval and on-demand input, which improves operational efficiency. Additionally, RAG allows for quick location of original sources, enhancing the model's ability to generate well-informed answers by integrating relevant external knowledge efficiently.\n",
      "\n",
      "=== Vector-based Response with image ===\n",
      "A primary challenge that Large Language Models (LLMs) face, which Retrieval-Augmented Generation (RAG) is designed to solve, is the efficient handling of large contexts and the need for precise information retrieval. While LLMs can manage extensive contexts, providing them with a large amount of context at once can significantly impact their inference speed. RAG addresses this by using chunked retrieval and on-demand input, which improves operational efficiency. Additionally, RAG-based generation can quickly locate the original sources of information, making it ideal for precise information retrieval tasks. This capability is particularly useful in dynamic environments where real-time knowledge updates and effective utilization of external knowledge sources are required.\n",
      "\n",
      "=== BM25 Response ===\n",
      "A primary challenge that large language models (LLMs) face, which Retrieval-Augmented Generation (RAG) is designed to solve, is the issue of hallucination and the retrieval of irrelevant or misaligned information. According to the text from page 3, during the retrieval phase, LLMs often struggle with precision and recall, leading to the selection of irrelevant chunks and missing crucial information. This can result in the generation of content that is not supported by the retrieved context, known as hallucination. RAG addresses these challenges by integrating retrieval mechanisms that enhance the relevance and accuracy of the information used by LLMs to generate responses.\n",
      "\n",
      "=== Fusion Response ===\n",
      "A primary challenge that large language models (LLMs) face, which Retrieval-Augmented Generation (RAG) is designed to solve, is the issue of hallucination during the generation phase. This occurs when the model produces content that is not supported by the retrieved context. Additionally, RAG addresses retrieval challenges, such as the precision and recall issues that lead to the selection of misaligned or irrelevant information and the omission of crucial data. By integrating retrieval mechanisms, RAG aims to provide LLMs with more accurate and relevant information, thereby enhancing the quality and reliability of the generated responses.\n",
      "\n",
      "=== Comparison ===\n",
      "### Evaluation of Responses\n",
      "\n",
      "#### 1. Vector-based Retrieval\n",
      "\n",
      "- **Relevance to the Query**: \n",
      "  - The response is somewhat relevant as it discusses the challenge of handling large contexts, which is a valid issue for LLMs. However, it does not directly address the primary challenge of hallucination, which is the focus of the reference answer.\n",
      "  \n",
      "- **Factual Correctness**: \n",
      "  - The information about handling large contexts and improving operational efficiency is correct but not directly aligned with the primary challenge of hallucination.\n",
      "  \n",
      "- **Comprehensiveness**: \n",
      "  - The response lacks comprehensiveness as it misses the key issue of hallucination, which is central to the query.\n",
      "  \n",
      "- **Clarity and Coherence**: \n",
      "  - The response is clear and coherent in its explanation of context handling but does not cover the main challenge effectively.\n",
      "\n",
      "#### 2. Vector-based Retrieval with Images\n",
      "\n",
      "- **Relevance to the Query**: \n",
      "  - Similar to the vector-based response, it focuses on context handling and precise information retrieval, which are relevant but not the primary challenge of hallucination.\n",
      "  \n",
      "- **Factual Correctness**: \n",
      "  - The response is factually correct regarding context handling and retrieval efficiency but misses the main point about hallucination.\n",
      "  \n",
      "- **Comprehensiveness**: \n",
      "  - It provides additional context about real-time knowledge updates, which adds depth but still does not address hallucination.\n",
      "  \n",
      "- **Clarity and Coherence**: \n",
      "  - The response is clear and coherent, with a slightly broader scope than the vector-based response.\n",
      "\n",
      "#### 3. BM25 Keyword Retrieval\n",
      "\n",
      "- **Relevance to the Query**: \n",
      "  - This response is highly relevant as it directly addresses the issue of hallucination and retrieval precision, aligning well with the reference answer.\n",
      "  \n",
      "- **Factual Correctness**: \n",
      "  - The response accurately describes the challenge of hallucination and the role of RAG in addressing it.\n",
      "  \n",
      "- **Comprehensiveness**: \n",
      "  - It comprehensively covers the key challenge of hallucination and retrieval issues, providing a well-rounded answer.\n",
      "  \n",
      "- **Clarity and Coherence**: \n",
      "  - The response is clear and coherent, effectively communicating the primary challenge and solution.\n",
      "\n",
      "#### 4. Fusion Retrieval\n",
      "\n",
      "- **Relevance to the Query**: \n",
      "  - The response is highly relevant, focusing on hallucination and retrieval challenges, which aligns well with the reference answer.\n",
      "  \n",
      "- **Factual Correctness**: \n",
      "  - It correctly identifies hallucination and retrieval precision as challenges addressed by RAG.\n",
      "  \n",
      "- **Comprehensiveness**: \n",
      "  - The response is comprehensive, covering both hallucination and retrieval issues, providing a thorough explanation.\n",
      "  \n",
      "- **Clarity and Coherence**: \n",
      "  - The response is clear and coherent, effectively addressing the query with a balanced explanation.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "- **Best Performing Approach**: **Fusion Retrieval**\n",
      "  - **Reason**: The fusion retrieval response is the most aligned with the reference answer, addressing both hallucination and retrieval challenges comprehensively and clearly. It combines the strengths of both vector and keyword approaches, providing a well-rounded and accurate response.\n",
      "\n",
      "- **Strengths and Weaknesses**:\n",
      "  - **Vector-based Retrieval**: Strong in explaining context handling but misses the primary challenge of hallucination.\n",
      "  - **Vector-based Retrieval with Images**: Adds depth with real-time updates but still misses the main challenge.\n",
      "  - **BM25 Keyword Retrieval**: Directly addresses hallucination and retrieval issues, aligning well with the reference answer.\n",
      "  - **Fusion Retrieval**: Combines the strengths of both approaches, providing a comprehensive and relevant response.\n",
      "\n",
      "\n",
      "=== OVERALL ANALYSIS ===\n",
      "\n",
      "### Comprehensive Analysis of Retrieval Approaches\n",
      "\n",
      "#### 1. Vector-based Retrieval (Text Only)\n",
      "\n",
      "- **Types of Queries Where It Performs Best**:\n",
      "  - This approach excels in queries that require deep semantic understanding and contextual relevance, such as those involving complex concepts or nuanced differences (e.g., Query 1 about Modular RAG advantages).\n",
      "  \n",
      "- **Strengths**:\n",
      "  - High semantic relevance due to the ability to understand context and meaning beyond keyword matching.\n",
      "  - Effective for queries that require understanding of complex relationships and abstract concepts.\n",
      "\n",
      "- **Weaknesses**:\n",
      "  - May miss out on relevant information if it is not semantically similar but still contextually important.\n",
      "  - Limited to text-based information, potentially missing out on valuable insights from non-textual data.\n",
      "\n",
      "#### 2. Vector-based Retrieval (Text and Image)\n",
      "\n",
      "- **Types of Queries Where It Performs Best**:\n",
      "  - Queries that can benefit from visual information or where images can provide additional context or clarification (e.g., Query 2 involving the evolution of RAG research with potential visual aids).\n",
      "\n",
      "- **Strengths**:\n",
      "  - Combines semantic understanding with the ability to incorporate visual data, providing a richer set of results.\n",
      "  - Useful for queries where images can enhance understanding or provide additional context.\n",
      "\n",
      "- **Weaknesses**:\n",
      "  - May introduce irrelevant image data if not properly filtered, potentially diluting the relevance of results.\n",
      "  - Requires more computational resources to process both text and image data.\n",
      "\n",
      "#### 3. BM25 Keyword Retrieval\n",
      "\n",
      "- **Types of Queries Where It Performs Best**:\n",
      "  - Simple, fact-based queries where specific keywords are crucial (e.g., Query 3 about primary challenges of LLMs).\n",
      "\n",
      "- **Strengths**:\n",
      "  - Fast and efficient for straightforward queries with clear keyword matches.\n",
      "  - Highly effective for retrieving documents with exact keyword matches.\n",
      "\n",
      "- **Weaknesses**:\n",
      "  - Lacks semantic understanding, potentially missing relevant documents that do not contain exact keywords.\n",
      "  - Not suitable for complex queries requiring deep contextual understanding.\n",
      "\n",
      "#### 4. Fusion Retrieval (Combination of Vector-based and BM25)\n",
      "\n",
      "- **Types of Queries Where It Performs Best**:\n",
      "  - Complex queries that benefit from both semantic understanding and precise keyword matching.\n",
      "  - Queries where both text and image data can provide comprehensive answers.\n",
      "\n",
      "- **Strengths**:\n",
      "  - Balances the strengths of both vector-based and keyword retrieval, providing a more holistic set of results.\n",
      "  - Can handle a wide range of query types, from simple to complex, by leveraging both semantic and keyword-based approaches.\n",
      "  - Incorporates visual data when relevant, enhancing the richness of the results.\n",
      "\n",
      "- **Weaknesses**:\n",
      "  - More computationally intensive due to the combination of multiple retrieval methods.\n",
      "  - Requires careful tuning to ensure the right balance between semantic and keyword relevance.\n",
      "\n",
      "#### Recommendations\n",
      "\n",
      "- **Vector-based Retrieval (Text Only)**: Use for queries requiring deep semantic understanding without the need for visual data.\n",
      "- **Vector-based Retrieval (Text and Image)**: Ideal for queries where visual context is beneficial or necessary.\n",
      "- **BM25 Keyword Retrieval**: Best for straightforward, fact-based queries with clear keyword targets.\n",
      "- **Fusion Retrieval**: Recommended for complex queries that can benefit from both semantic and keyword-based approaches, especially when visual data can enhance the results.\n",
      "\n",
      "### Summary Table\n",
      "\n",
      "| Retrieval Approach                  | Best For Queries Involving                  | Overall Strengths                          | Overall Weaknesses                        |\n",
      "|-------------------------------------|---------------------------------------------|--------------------------------------------|-------------------------------------------|\n",
      "| Vector-based (Text Only)            | Complex concepts, nuanced differences       | High semantic relevance                    | Limited to text, may miss non-semantic info|\n",
      "| Vector-based (Text and Image)       | Visual context, complex queries             | Rich results with text and images          | Potentially irrelevant images, resource-intensive |\n",
      "| BM25 Keyword Retrieval              | Simple, fact-based queries                  | Fast, efficient keyword matching           | Lacks semantic understanding               |\n",
      "| Fusion Retrieval                    | Complex, multi-faceted queries              | Balances semantic and keyword strengths    | Computationally intensive                  |\n",
      "\n",
      "Overall, **Fusion Retrieval** is the most versatile and effective approach for a wide range of queries, particularly those that are complex and benefit from both semantic understanding and precise keyword matching, as well as visual data when applicable.\n"
     ]
    }
   ],
   "source": [
    "# Path to validation document with questions and answers\n",
    "\n",
    "# Path to the PDF document to be evaluated\n",
    "pdf_path = \"data/RAG_white_papers_articles.pdf\"\n",
    "val_path = \"data/val.json\"\n",
    "generate_validation_questions_anaswers(pdf_path, num_question_answers=10)\n",
    "validation_doc = json.load(open(val_path, \"r\"))\n",
    "\n",
    "# test_queries = [item['question'] for item in validation_doc]\n",
    "\n",
    "## Quick test queries\n",
    "test_queries = [\n",
    "    \"Why might Modular RAG offer a significant advantage over Naive or Advanced RAG in real-world applications?\",\n",
    "    \"Based on Figure 1 (RAG Technology Tree), how has RAG research evolved over time?\",  # AI-specific query\n",
    "    \"What is a primary challenge that LLMs face which RAG is designed to solve?\"\n",
    "]\n",
    "\n",
    "# Optional reference answer\n",
    "\n",
    "# reference_answers = [item['answer'] for item in validation_doc]\n",
    "\n",
    "reference_answers = [\n",
    "    \"Modular RAG provides flexibility by allowing modules to be rearranged or replaced, enabling better adaptation to diverse tasks, reducing redundancy, and supporting more dynamic interaction flows\",\n",
    "    \"Initially focused on inference through retrieval, RAG research has expanded into pre-training and fine-tuning stages, incorporating more complex and adaptable architectures\",\n",
    "    \"Hallucination—producing content not grounded in factual sources—is a key challenge RAG addresses by grounding generation in retrieved external knowledge\"\n",
    "\n",
    "]\n",
    "\n",
    "# Set parameters\n",
    "k = 5  # Number of documents to retrieve\n",
    "alpha = 0.5  # Weight for vector scores (0.5 means equal weight between vector and BM25)\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results = evaluate_fusion_retrieval(\n",
    "    pdf_path=pdf_path,\n",
    "    test_queries=test_queries,\n",
    "    reference_answers=reference_answers,\n",
    "    k=k,\n",
    "    alpha=alpha\n",
    ")\n",
    "\n",
    "# Print overall analysis\n",
    "print(\"\\n\\n=== OVERALL ANALYSIS ===\\n\")\n",
    "print(evaluation_results[\"overall_analysis\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
