[
    {
        "question": "What is the main challenge faced by Large Language Models (LLMs)?",
        "answer": "LLMs face challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes.",
        "type": "factual"
    },
    {
        "question": "What is the purpose of Retrieval-Augmented Generation (RAG)?",
        "answer": "RAG enhances LLMs by retrieving relevant document chunks from external knowledge bases through semantic similarity calculation.",
        "type": "factual"
    },
    {
        "question": "How does RAG improve the accuracy and credibility of LLM generation?",
        "answer": "RAG integrates knowledge from external databases, improving the accuracy and credibility of LLM generation, particularly for knowledge-intensive tasks.",
        "type": "factual"
    },
    {
        "question": "What is the significance of the \"Naive RAG\" and \"Advanced RAG\" paradigms?",
        "answer": "The Naive RAG and Advanced RAG are two stages of development in RAG, representing a progression of techniques and approaches.",
        "type": "factual"
    },
    {
        "question": "What is the role of the \"Modular RAG\" framework?",
        "answer": "The Modular RAG framework allows for the integration of different components, such as retrieval, generation, and augmentation, to create a more robust RAG system.",
        "type": "factual"
    },
    {
        "question": "Why is the integration of external knowledge into LLMs crucial for addressing the limitations of LLMs?",
        "answer": "Integrating external knowledge helps LLMs overcome limitations like hallucination, outdated knowledge, and non-transparent reasoning by providing accurate and up-to-date information.",
        "type": "analytical"
    },
    {
        "question": "Why might Modular RAG offer a significant advantage over Naive or Advanced RAG in real-world applications?",
        "answer": "The document states that RAG research has expanded beyond the inference stage to incorporate LLM fine-tuning techniques, indicating a broader scope of application.",
        "type": "analytical"
    },
    {
        "question": "How does the encoding help to solve the lack of order of the input sequence?",
        "answer": "This is done by adding positional encodings to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings in the Transformer model is to inject information about the relative or absolute position of the tokens in the sequence. The positional encodings have the same dimensionality as the embeddings, so they can be summed together. By using sine and cosine functions of different frequencies, the positional encodings form a geometric progression from 2π to 10000 · 2π. This allows the model to easily learn to attend by relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of PEpos",
        "type": "analytical"
    },
    {
        "question": "Why is the development of RAG considered a significant advancement in LLM research?",
        "answer": "RAG is a significant advancement because it addresses the limitations of LLMs by providing them with access to external knowledge, enhancing their accuracy, credibility, and ability to handle complex tasks.",
        "type": "analytical"
    },
    {
        "question": "How does the document suggest that RAG research is still evolving?",
        "answer": "The document highlights the challenges faced by RAG research, such as the need for better evaluation frameworks and benchmarks, indicating that the field is still evolving and undergoing continuous development.",
        "type": "analytical"
    }
]